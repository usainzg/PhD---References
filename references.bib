
@inproceedings{zheng_ansor_2020,
	title = {Ansor : {Generating} {High}-{Performance} {Tensor} {Programs} for {Deep} {Learning}},
	shorttitle = {Ansor},
	url = {https://www.semanticscholar.org/paper/Ansor-%3A-Generating-High-Performance-Tensor-Programs-Zheng-Jia/6ebbdc79a3553e8ad2996d6d03987bfaeb448e82},
	abstract = {High-performance tensor programs are crucial to guarantee efficient execution of deep learning models. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously difficult. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering efforts in developing platform-specific optimization code or fall short in finding high-performance programs due to restricted search space and ineffective exploration strategy. 
We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores much more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. Besides, Ansor utilizes a scheduler to simultaneously optimize multiple subgraphs in a set of deep neural networks. Our evaluation shows that Ansor improves the execution performance of deep neural networks on the Intel CPU, ARM CPU, and NVIDIA GPU by up to \$3.8{\textbackslash}times\$, \$2.6{\textbackslash}times\$, and \$1.7 {\textbackslash}times\$, respectively.},
	urldate = {2023-10-03},
	author = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, I.},
	month = jun,
	year = {2020},
}

@article{ding_hidet_2023,
	title = {Hidet: {Task}-{Mapping} {Programming} {Paradigm} for {Deep} {Learning} {Tensor} {Programs}},
	shorttitle = {Hidet},
	url = {https://dl.acm.org/doi/10.1145/3575693.3575702},
	doi = {10.1145/3575693.3575702},
	abstract = {As deep learning models nowadays are widely adopted by both cloud services and edge devices, reducing the latency of deep learning model inferences becomes crucial to provide efficient model serving. However, it is challenging to develop efficient tensor programs for deep learning operators due to the high complexity of modern accelerators (e.g., NVIDIA GPUs and Google TPUs) and the rapidly growing number of operators. Deep learning compilers, such as Apache TVM, adopt declarative scheduling primitives to lower the bar of developing tensor programs. However, we show that this approach is insufficient to cover state-of-the-art tensor program optimizations (e.g., double buffering). In this paper, we propose to embed the scheduling process into tensor programs and use dedicated mappings, called task mappings, to define the computation assignment and ordering directly in the tensor programs. This new approach greatly enriches the expressible optimizations by allowing developers to manipulate tensor programs at a much finer granularity (e.g., allowing program-statement-level optimizations). We call the proposed method the task-mapping programming paradigm. In addition, we propose a new post-scheduling fusion optimization that allows developers to focus on scheduling every single operator and automates the fusion after scheduling. It greatly reduces the engineering efforts for operator fusion. Our proposed paradigm also constructs an efficient hardware-centric schedule space, which is agnostic to the program input size and greatly reduces the tuning time. With the proposed paradigm, we implement a deep learning compiler Hidet. Extensive experiments on modern convolution and transformer models show that Hidet outperforms state-of-the-art DNN inference framework, ONNX Runtime, and compiler, TVM equipped with scheduler AutoTVM and Ansor, by up to 1.48x (1.22x on average). It also reduces the tuning time by 20x and 11x compared with AutoTVM and Ansor, respectively. We open-sourced hidet at https://www.github.com/hidet-org/hidet.},
	language = {en},
	urldate = {2023-10-03},
	journal = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
	author = {Ding, Yaoyao and Yu, Cody Hao and Zheng, Bojian and Liu, Yizhi and Wang, Yida and Pekhimenko, Gennady},
	month = jan,
	year = {2023},
	note = {Conference Name: ASPLOS '23: 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2
ISBN: 9781450399166
Place: Vancouver BC Canada
Publisher: ACM},
	pages = {370--384},
}

@article{kim_code_2019,
	title = {A {Code} {Generator} for {High}-{Performance} {Tensor} {Contractions} on {GPUs}},
	url = {https://ieeexplore.ieee.org/document/8661182/},
	doi = {10.1109/CGO.2019.8661182},
	abstract = {Tensor contractions are higher dimensional generalizations of matrix-matrix multiplication. They form the compute-intensive core of many applications in computational science and data science. In this paper, we describe a high-performance GPU code generator for arbitrary tensor contractions. It exploits domain-specific properties about data reuse in tensor contractions to devise an effective code generation schema, coupled with an effective model-driven search, to determine parameters for mapping of computation to threads and staging of data through the GPU memory hierarchy. Experimental evaluation using a set of tensor contraction benchmarks demonstrates performance improvement and/or significantly reduced code generation time over other state-of-the-art tensor contraction libraries and code generators.},
	urldate = {2023-10-03},
	journal = {2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
	author = {Kim, Jinsung and Sukumaran-Rajam, Aravind and Thumma, Vineeth and Krishnamoorthy, Sriram and Panyala, Ajay and Pouchet, Louis-Noel and Rountev, Atanas and Sadayappan, P.},
	month = feb,
	year = {2019},
	note = {Conference Name: 2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)
ISBN: 9781728114361
Place: Washington, DC, USA
Publisher: IEEE},
	pages = {85--95},
}

@article{paliwal_reinforced_2019,
	title = {Reinforced {Genetic} {Algorithm} {Learning} for {Optimizing} {Computation} {Graphs}},
	url = {https://www.semanticscholar.org/paper/Reinforced-Genetic-Algorithm-Learning-for-Graphs-Paliwal-Gimeno/bc48ecef55aeb5a31899b1995d7ae95da40ddb73},
	abstract = {We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.},
	urldate = {2023-10-03},
	journal = {arXiv: Learning},
	author = {Paliwal, Aditya Sanjay and Gimeno, Felix and Nair, Vinod and Li, Yujia and Lubin, Miles and Kohli, Pushmeet and Vinyals, Oriol},
	month = may,
	year = {2019},
	keywords = {to read},
}

@article{feng_tensorir_2023,
	title = {{TensorIR}: {An} {Abstraction} for {Automatic} {Tensorized} {Program} {Optimization}},
	shorttitle = {{TensorIR}},
	url = {https://dl.acm.org/doi/10.1145/3575693.3576933},
	doi = {10.1145/3575693.3576933},
	abstract = {Deploying deep learning models on various devices has become an important topic. The wave of hardware specialization brings a diverse set of acceleration primitives for multi-dimensional ten- sor computations. These new acceleration primitives, along with the emerging machine learning models, bring tremendous engineering challenges. In this paper, we present TensorIR, a compiler abstraction for optimizing programs with these tensor computation primitives. TensorIR generalizes the loop nest representation used in existing machine learning compilers to bring tensor computation as the first-class citizen. Finally, we build an end-to-end framework on top of our abstraction to automatically optimize deep learning models for given tensor computation primitives. Experimental results show that TensorIR compilation automatically uses the tensor computation primitives for given hardware backends and delivers performance that is competitive to state-of-art hand-optimized systems across platforms.},
	language = {en},
	urldate = {2023-10-03},
	journal = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
	author = {Feng, Siyuan and Hou, Bohan and Jin, Hongyi and Lin, Wuwei and Shao, Junru and Lai, Ruihang and Ye, Zihao and Zheng, Lianmin and Yu, Cody Hao and Yu, Yong and Chen, Tianqi},
	month = jan,
	year = {2023},
	note = {Conference Name: ASPLOS '23: 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2
ISBN: 9781450399166
Place: Vancouver BC Canada
Publisher: ACM},
	keywords = {to read},
	pages = {804--817},
}

@article{gysi_domain-specific_2021,
	title = {Domain-{Specific} {Multi}-{Level} {IR} {Rewriting} for {GPU}: {The} {Open} {Earth} {Compiler} for {GPU}-accelerated {Climate} {Simulation}},
	volume = {18},
	issn = {1544-3566, 1544-3973},
	shorttitle = {Domain-{Specific} {Multi}-{Level} {IR} {Rewriting} for {GPU}},
	url = {https://dl.acm.org/doi/10.1145/3469030},
	doi = {10.1145/3469030},
	abstract = {Most compilers have a single core intermediate representation (IR) (e.g., LLVM) sometimes complemented with vaguely defined IR-like data structures. This IR is commonly low-level and close to machine instructions. As a result, optimizations relying on domain-specific information are either not possible or require complex analysis to recover the missing information. In contrast, multi-level rewriting instantiates a hierarchy of dialects (IRs), lowers programs level-by-level, and performs code transformations at the most suitable level. We demonstrate the effectiveness of this approach for the weather and climate domain. In particular, we develop a prototype compiler and design stencil- and GPU-specific dialects based on a set of newly introduced design principles. We find that two domain-specific optimizations (500 lines of code) realized on top of LLVM’s extensible MLIR compiler infrastructure suffice to outperform state-of-the-art solutions. In essence, multi-level rewriting promises to herald the age of specialized compilers composed from domain- and target-specific dialects implemented on top of a shared infrastructure.},
	language = {en},
	number = {4},
	urldate = {2023-10-02},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Gysi, Tobias and Müller, Christoph and Zinenko, Oleksandr and Herhut, Stephan and Davis, Eddie and Wicky, Tobias and Fuhrer, Oliver and Hoefler, Torsten and Grosser, Tobias},
	month = dec,
	year = {2021},
	keywords = {done},
	pages = {1--23},
}

@article{li_deep_2021,
	title = {The {Deep} {Learning} {Compiler}: {A} {Comprehensive} {Survey}},
	volume = {32},
	issn = {1558-2183},
	shorttitle = {The {Deep} {Learning} {Compiler}},
	doi = {10.1109/TPDS.2020.3030548},
	abstract = {The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.},
	number = {3},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
	month = mar,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {!!!, Computational modeling, Computer architecture, Deep learning, Hardware, Integrated circuit modeling, Libraries, Neural networks, Optimization, compiler, deep learning, intermediate representation, optimization},
	pages = {708--727},
}

@article{hijma_optimization_2023,
	title = {Optimization {Techniques} for {GPU} {Programming}},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3570638},
	doi = {10.1145/3570638},
	abstract = {In the past decade, Graphics Processing Units have played an important role in the field of high-performance computing and they still advance new fields such as IoT, autonomous vehicles, and exascale computing. It is therefore important to understand how to extract performance from these processors, something that is not trivial. This survey discusses various optimization techniques found in 450 articles published in the last 14 years. We analyze the optimizations from different perspectives which shows that the various optimizations are highly interrelated, explaining the need for techniques such as auto-tuning.},
	number = {11},
	urldate = {2023-10-02},
	journal = {ACM Computing Surveys},
	author = {Hijma, Pieter and Heldens, Stijn and Sclocco, Alessio and van Werkhoven, Ben and Bal, Henri E.},
	month = mar,
	year = {2023},
	keywords = {!!!, GPU, Survey, optimization, optimization techniques, performance bottleneck},
	pages = {239:1--239:81},
}

@misc{huang_alcop_2023,
	title = {{ALCOP}: {Automatic} {Load}-{Compute} {Pipelining} in {Deep} {Learning} {Compiler} for {AI}-{GPUs}},
	shorttitle = {{ALCOP}},
	url = {http://arxiv.org/abs/2210.16691},
	doi = {10.48550/arXiv.2210.16691},
	abstract = {Pipelining between data loading and computation is a critical tensor program optimization for GPUs. In order to unleash the high performance of latest GPUs, we must perform a synergetic optimization of multi-stage pipelining across the multi-level buffer hierarchy of GPU. Existing frameworks rely on hand-written libraries such as cuBLAS to perform pipelining optimization, which is inextensible to new operators and un-composable with prior tensor compiler optimizations. This paper presents ALCOP, the first framework that is compiler-native and fully supports multi-stage multi-level pipelining. ALCOP overcomes three critical obstacles in generating code for pipelining: detection of pipelining-applicable buffers, program transformation for multi-level multi-stage pipelining, and efficient schedule parameter search by incorporating static analysis. Experiments show that ALCOP can generate programs with 1.23x speedup on average (up to 1.73x) over vanilla TVM. On end-to-end models, ALCOP can improve upon TVM by up to 1.18x, and XLA by up to 1.64x. Besides, our performance model significantly improves the efficiency of the schedule tuning process and can find schedules with 99\% of the performance given by exhaustive search while costing 40x fewer trials.},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Huang, Guyue and Bai, Yang and Liu, Liu and Wang, Yuke and Yu, Bei and Ding, Yufei and Xie, Yuan},
	month = may,
	year = {2023},
	note = {arXiv:2210.16691 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, done},
}

@article{tavarageri_polyscientist_2020,
	title = {{PolyScientist}: {Automatic} {Loop} {Transformations} {Combined} with {Microkernels} for {Optimization} of {Deep} {Learning} {Primitives}},
	shorttitle = {{PolyScientist}},
	url = {https://www.semanticscholar.org/paper/PolyScientist%3A-Automatic-Loop-Transformations-with-Tavarageri-Heinecke/4ec0f84c56a6902709810c6eec5037bba1e73a70},
	abstract = {At the heart of deep learning training and inferencing are computationally intensive primitives such as convolutions which form the building blocks of deep neural networks. Researchers have taken two distinct approaches to creating high performance implementations of deep learning kernels, namely, 1) library development exemplified by Intel MKLDNN for CPUs, 2) automatic compilation represented by the TensorFlow XLA compiler. The two approaches have their drawbacks: even though a custom built library can deliver very good performance, the cost and time of development of the library can be high. Additionally, hand coding of a plethora of operators for performance is not scalable over the long term as more and more deep learning operators get invented. Automatic compilation of kernels is attractive but in practice, till date, automatically generated implementations lag expert coded kernels in performance by orders of magnitude. In this paper, we develop a hybrid solution to the development of deep learning kernels that achieves the best of both worlds: the expert coded microkernels are utilized for the innermost loops of kernels that exploit the vector register files, and vector units of modern CPUs effectively, and we use the advanced polyhedral compilation technology to automatically tune the outer loops for performance. We design a novel polyhedral model based data reuse algorithm to optimize the outer loops of the kernel. The overall effect of this combined approach will be that 1) the library development effort is reduced to writing of only a small number of tiny kernels that occur commonly in deep learning workloads, and thus library development is made scalable; 2) automatic compilation with the use of expert-coded microkernels will achieve state-of-the art high performance. Through experimental evaluation on an important class of deep learning primitives namely convolutions, we demonstrate that the approach we develop attains the same levels of performance as Intel MKL-DNN, a hand coded deep learning library.},
	urldate = {2023-10-02},
	journal = {ArXiv},
	author = {Tavarageri, Sanket and Heinecke, A. and Avancha, Sasikanth and Goyal, Gagandeep and Upadrasta, Ramakrishna and Kaul, Bharat},
	month = feb,
	year = {2020},
	keywords = {done},
}

@article{grubisic_looptune_2023,
	title = {{LoopTune}: {Optimizing} {Tensor} {Computations} with {Reinforcement} {Learning}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{LoopTune}},
	url = {https://arxiv.org/abs/2309.01825},
	doi = {10.48550/ARXIV.2309.01825},
	abstract = {Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.},
	urldate = {2023-10-02},
	author = {Grubisic, Dejan and Wasti, Bram and Cummins, Chris and Mellor-Crummey, John and Zlateski, Aleksandar},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Programming Languages (cs.PL)},
}

@inproceedings{ikarashi_exocompilation_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Exocompilation for productive programming of hardware accelerators},
	isbn = {978-1-4503-9265-5},
	url = {https://dl.acm.org/doi/10.1145/3519939.3523446},
	doi = {10.1145/3519939.3523446},
	abstract = {High-performance kernel libraries are critical to exploiting accelerators and specialized instructions in many applications. Because compilers are difficult to extend to support diverse and rapidly-evolving hardware targets, and automatic optimization is often insufficient to guarantee state-of-the-art performance, these libraries are commonly still coded and optimized by hand, at great expense, in low-level C and assembly. To better support development of high-performance libraries for specialized hardware, we propose a new programming language, Exo, based on the principle of exocompilation: externalizing target-specific code generation support and optimization policies to user-level code. Exo allows custom hardware instructions, specialized memories, and accelerator configuration state to be defined in user libraries. It builds on the idea of user scheduling to externalize hardware mapping and optimization decisions. Schedules are defined as composable rewrites within the language, and we develop a set of effect analyses which guarantee program equivalence and memory safety through these transformations. We show that Exo enables rapid development of state-of-the-art matrix-matrix multiply and convolutional neural network kernels, for both an embedded neural accelerator and x86 with AVX-512 extensions, in a few dozen lines of code each.},
	urldate = {2023-09-24},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ikarashi, Yuka and Bernstein, Gilbert Louis and Reinking, Alex and Genc, Hasan and Ragan-Kelley, Jonathan},
	month = jun,
	year = {2022},
	keywords = {hardware accelerators, instruction abstraction, program optimization, scheduling, user-extensible backend \& scheduling, user-schedulable languages},
	pages = {703--718},
}

@article{alaejos_micro-kernels_2023,
	title = {Micro-kernels for portable and efficient matrix multiplication in deep learning},
	volume = {79},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-022-05003-3},
	doi = {10.1007/s11227-022-05003-3},
	abstract = {We provide a practical demonstration that it is possible to systematically generate a variety of high-performance micro-kernels for the general matrix multiplication (gemm) via generic templates which can be easily customized to different processor architectures and micro-kernel dimensions. These generic templates employ vector intrinsics to exploit the SIMD (single instruction, multiple data) units in current general-purpose processors and, for the particular type of gemm problems encountered in deep learning, deliver a floating-point throughput rate on par with or even higher than that obtained with conventional, carefully tuned implementations of gemm in current linear algebra libraries (e.g., BLIS, AMD AOCL, ARMPL). Our work exposes the structure of the template-based micro-kernels for ARM Neon (128-bit SIMD), ARM SVE (variable-length SIMD) and Intel AVX512 (512-bit SIMD), showing considerable performance for an NVIDIA Carmel processor (ARM Neon), a Fujitsu A64FX processor (ARM SVE) and on an AMD EPYC 7282 processor (256-bit SIMD).},
	language = {en},
	number = {7},
	urldate = {2023-09-23},
	journal = {The Journal of Supercomputing},
	author = {Alaejos, Guillermo and Castelló, Adrián and Martínez, Héctor and Alonso-Jordá, Pedro and Igual, Francisco D. and Quintana-Ortí, Enrique S.},
	month = may,
	year = {2023},
	keywords = {High performance, Linear algebra libraries, Matrix multiplication, SIMD units, Vector intrinsics},
	pages = {8124--8147},
}

@misc{cummins_large_2023,
	title = {Large {Language} {Models} for {Compiler} {Optimization}},
	url = {http://arxiv.org/abs/2309.07062},
	doi = {10.48550/arXiv.2309.07062},
	abstract = {We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0\% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91\% of the time and perfectly emulating the output of the compiler 70\% of the time.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Elhoushi, Mostafa and Liang, Youwei and Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Hazelwood, Kim and Synnaeve, Gabriel and Leather, Hugh},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{lattner_mlir_2020,
	title = {{MLIR}: {A} {Compiler} {Infrastructure} for the {End} of {Moore}'s {Law}},
	shorttitle = {{MLIR}},
	url = {http://arxiv.org/abs/2002.11054},
	doi = {10.48550/arXiv.2002.11054},
	abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
	urldate = {2023-09-16},
	publisher = {arXiv},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	month = feb,
	year = {2020},
	note = {arXiv:2002.11054 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@article{tillet_triton_2019,
	title = {Triton: an intermediate language and compiler for tiled neural network computations},
	shorttitle = {Triton},
	url = {https://dl.acm.org/doi/10.1145/3315508.3329973},
	doi = {10.1145/3315508.3329973},
	abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
	language = {en},
	urldate = {2023-09-16},
	journal = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
	author = {Tillet, Philippe and Kung, H. T. and Cox, David},
	month = jun,
	year = {2019},
	note = {Conference Name: PLDI '19: 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
ISBN: 9781450367196
Place: Phoenix AZ USA
Publisher: ACM},
	pages = {10--19},
}

@inproceedings{shen_comprehensive_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {A comprehensive study of deep learning compiler bugs},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468591},
	doi = {10.1145/3468264.3468591},
	abstract = {There are increasing uses of deep learning (DL) compilers to generate optimized code, boosting the runtime performance of DL models on specific hardware. Like their traditional counterparts, DL compilers can generate incorrect code, resulting in unexpected model behaviors that may cause catastrophic consequences in mission-critical systems. On the other hand, the DL models processed by DL compilers differ fundamentally from imperative programs in that the program logic in DL models is implicit. As such, various characteristics of the bugs arising from traditional compilers need to be revisited in the context of DL compilers. In this paper, we present the first systematic study of DL compiler bugs by analyzing 603 bugs arising in three popular DL compilers (i.e., TVM from Apache, Glow from Facebook, and nGraph from Intel). We analyzed these bugs according to their root causes, symptoms, and the stages where they occur during compilation. We obtain 12 findings, and provide a series of valuable guidelines for future work on DL compiler bug detection and debugging. For example, a large portion (nearly 20\%) of DL compiler bugs are related to types, especially tensor types. The analysis of these bugs helps design new mutation operators (e.g., adding type cast for a tensor to promote implicit type conversion in subsequent tensor computations) to facilitate type-related bug detection. Further, we developed TVMfuzz as a proof-of-concept application of our findings to test the TVM DL compiler. It generates new tests based on TVM's original test suite. They expose 8 TVM bugs that are missed by the original test suite. The result demonstrates the usefulness of our findings.},
	urldate = {2023-09-15},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Shen, Qingchao and Ma, Haoyang and Chen, Junjie and Tian, Yongqiang and Cheung, Shing-Chi and Chen, Xiang},
	year = {2021},
	keywords = {Compiler Testing, Deep Learning, Deep Learning Compiler Bug, Empirical Study},
	pages = {968--980},
}

@inproceedings{zheng_amos_2022,
	address = {New York, NY, USA},
	series = {{ISCA} '22},
	title = {{AMOS}: enabling {\textless}u{\textgreater}a{\textless}/u{\textgreater}utomatic {\textless}u{\textgreater}m{\textless}/u{\textgreater}apping for tensor computations {\textless}u{\textgreater}o{\textless}/u{\textgreater}n {\textless}u{\textgreater}s{\textless}/u{\textgreater}patial accelerators with hardware abstraction},
	isbn = {978-1-4503-8610-4},
	shorttitle = {{AMOS}},
	url = {https://dl.acm.org/doi/10.1145/3470496.3527440},
	doi = {10.1145/3470496.3527440},
	abstract = {Hardware specialization is a promising trend to sustain performance growth. Spatial hardware accelerators that employ specialized and hierarchical computation and memory resources have recently shown high performance gains for tensor applications such as deep learning, scientific computing, and data mining. To harness the power of these hardware accelerators, programmers have to use specialized instructions with certain hardware constraints. However, these hardware accelerators and instructions are quite new and there is a lack of understanding of the hardware abstraction, performance optimization space, and automatic methodologies to explore the space. Existing compilers use hand-tuned computation implementations and optimization templates, resulting in sub-optimal performance and heavy development costs. In this paper, we propose AMOS, which is an automatic compilation framework for spatial hardware accelerators. Central to this framework is the hardware abstraction that not only clearly specifies the behavior of spatial hardware instructions, but also formally defines the mapping problem from software to hardware. Based on the abstraction, we develop algorithms and performance models to explore various mappings automatically. Finally, we build a compilation framework that uses the hardware abstraction as compiler intermediate representation (IR), explores both compute mappings and memory mappings, and generates high-performance code for different hardware backends. Our experiments show that AMOS achieves more than 2.50× speedup to hand-optimized libraries on Tensor Core, 1.37× speedup to TVM on vector units of Intel CPU for AVX-512, and up to 25.04× speedup to AutoTVM on dot units of Mali GPU. The source code of AMOS is publicly available.},
	urldate = {2023-09-15},
	booktitle = {Proceedings of the 49th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Size and Chen, Renze and Wei, Anjiang and Jin, Yicheng and Han, Qin and Lu, Liqiang and Wu, Bingyang and Li, Xiuhong and Yan, Shengen and Liang, Yun},
	month = jun,
	year = {2022},
	keywords = {code generation, mapping, spatial accelerators, tensor computations},
	pages = {874--887},
}

@misc{tian_high-performance_2021,
	title = {A {High}-{Performance} {Sparse} {Tensor} {Algebra} {Compiler} in {Multi}-{Level} {IR}},
	url = {http://arxiv.org/abs/2102.05187},
	doi = {10.48550/arXiv.2102.05187},
	abstract = {Tensor algebra is widely used in many applications, such as scientific computing, machine learning, and data analytics. The tensors represented real-world data are usually large and sparse. There are tens of storage formats designed for sparse matrices and/or tensors and the performance of sparse tensor operations depends on a particular architecture and/or selected sparse format, which makes it challenging to implement and optimize every tensor operation of interest and transfer the code from one architecture to another. We propose a tensor algebra domain-specific language (DSL) and compiler infrastructure to automatically generate kernels for mixed sparse-dense tensor algebra operations, named COMET. The proposed DSL provides high-level programming abstractions that resemble the familiar Einstein notation to represent tensor algebra operations. The compiler performs code optimizations and transformations for efficient code generation while covering a wide range of tensor storage formats. COMET compiler also leverages data reordering to improve spatial or temporal locality for better performance. Our results show that the performance of automatically generated kernels outperforms the state-of-the-art sparse tensor algebra compiler, with up to 20.92x, 6.39x, and 13.9x performance improvement, for parallel SpMV, SpMM, and TTM over TACO, respectively.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Tian, Ruiqin and Guo, Luanzheng and Li, Jiajia and Ren, Bin and Kestor, Gokcen},
	month = feb,
	year = {2021},
	note = {arXiv:2102.05187 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
}

@inproceedings{li_adatune_2020,
	title = {{AdaTune}: {Adaptive} {Tensor} {Program} {Compilation} {Made} {Efficient}},
	volume = {33},
	shorttitle = {{AdaTune}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html},
	abstract = {Deep learning models are computationally intense, and implementations often have to be highly optimized by experts or hardware vendors to be usable in practice. The DL compiler, together with Learning to Compile have proven to be a powerful technique for optimizing tensor programs. However, a limitation of this approach is that it still suffers from unbearably long overall optimization time.},
	urldate = {2023-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Menghao and Zhang, Minjia and Wang, Chi and Li, Mingqin},
	year = {2020},
	pages = {14807--14819},
}

@inproceedings{ahrens_autoscheduling_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Autoscheduling for sparse tensor algebra with an asymptotic cost model},
	isbn = {978-1-4503-9265-5},
	url = {https://dl.acm.org/doi/10.1145/3519939.3523442},
	doi = {10.1145/3519939.3523442},
	abstract = {While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small Pareto frontier of asymptotically non-dominating kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.},
	urldate = {2023-09-13},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Peter and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = jun,
	year = {2022},
	keywords = {Asymptotic Analysis, Automatic Scheduling, Compilers, Conjunctive Query Containment, Query Optimization, Sparse Tensors},
	pages = {269--285},
}

@article{chou_compilation_2022,
	title = {Compilation of dynamic sparse tensor algebra},
	volume = {6},
	url = {https://dl.acm.org/doi/10.1145/3563338},
	doi = {10.1145/3563338},
	abstract = {Many applications, from social network graph analytics to control flow analysis, compute on sparse data that evolves over the course of program execution. Such data can be represented as dynamic sparse tensors and efficiently stored in formats (data layouts) that utilize pointer-based data structures like block linked lists, binary search trees, B-trees, and C-trees among others. These specialized formats support fast in-place modification and are thus better suited than traditional, array-based data structures like CSR for storing dynamic sparse tensors. However, different dynamic sparse tensor formats have distinct benefits and drawbacks, and performing different computations on tensors that are stored in different formats can require vastly dissimilar code that are not straightforward to correctly implement and optimize. This paper shows how a compiler can generate efficient code to compute tensor algebra operations on dynamic sparse tensors that may be stored in a wide range of disparate formats. We propose a language for precisely specifying recursive, pointer-based data structures, and we show how this language can express many different dynamic data structures, including all the ones named above as well as many more. We then describe how, given high-level specifications of such dynamic data structures, a compiler can emit code to efficiently access and compute on dynamic sparse tensors that are stored in the aforementioned data structures. We evaluate our technique and find it generates efficient dynamic sparse tensor algebra kernels that have performance comparable to, if not better than, state-of-the-art libraries and frameworks such as PAM, Aspen, STINGER, and Terrace. At the same time, our technique supports a wider range of tensor algebra operations---such as those that simultaneously compute with static and dynamic sparse tensors---than Aspen, STINGER, and Terrace, while also achieving significantly better performance than PAM for those same operations.},
	number = {OOPSLA2},
	urldate = {2023-09-13},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Chou, Stephen and Amarasinghe, Saman},
	month = oct,
	year = {2022},
	keywords = {dynamic sparse tensors, node schema language, pointer-based data structures, sparse tensor algebra, sparse tensor algebra compilation, sparse tensor formats},
	pages = {175:1408--175:1437},
}

@article{kjolstad_tensor_2017,
	title = {The tensor algebra compiler},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3133901},
	doi = {10.1145/3133901},
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	number = {OOPSLA},
	urldate = {2023-09-13},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
	month = oct,
	year = {2017},
	keywords = {code generation, iteration graphs, linear algebra, merge lattices, parallelism, performance, sparse data structures, tensor algebra, tensors},
	pages = {77:1--77:29},
}

@article{bik_compiler_2022,
	title = {Compiler {Support} for {Sparse} {Tensor} {Computations} in {MLIR}},
	volume = {19},
	issn = {1544-3566, 1544-3973},
	url = {http://arxiv.org/abs/2202.04305},
	doi = {10.1145/3544559},
	abstract = {Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This paper discusses integrating this idea into MLIR.},
	number = {4},
	urldate = {2023-09-13},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Bik, Aart J. C. and Koanantakool, Penporn and Shpeisman, Tatiana and Vasilache, Nicolas and Zheng, Bixia and Kjolstad, Fredrik},
	month = dec,
	year = {2022},
	note = {arXiv:2202.04305 [cs]},
	keywords = {Computer Science - Programming Languages},
	pages = {1--25},
}

@misc{tian_gpu_2023,
	title = {{GPU} {First} -- {Execution} of {Legacy} {CPU} {Codes} on {GPUs}},
	url = {http://arxiv.org/abs/2306.11686},
	doi = {10.48550/arXiv.2306.11686},
	abstract = {Utilizing GPUs is critical for high performance on heterogeneous systems. However, leveraging the full potential of GPUs for accelerating legacy CPU applications can be a challenging task for developers. The porting process requires identifying code regions amenable to acceleration, managing distinct memories, synchronizing host and device execution, and handling library functions that may not be directly executable on the device. This complexity makes it challenging for non-experts to leverage GPUs effectively, or even to start offloading parts of a large legacy application. In this paper, we propose a novel compilation scheme called "GPU First" that automatically compiles legacy CPU applications directly for GPUs without any modification of the application source. Library calls inside the application are either resolved through our partial libc GPU implementation or via automatically generated remote procedure calls to the host. Our approach simplifies the task of identifying code regions amenable to acceleration and enables rapid testing of code modifications on actual GPU hardware in order to guide porting efforts. Our evaluation on two HPC proxy applications with OpenMP CPU and GPU parallelism, four micro benchmarks with originally GPU only parallelism, as well as three benchmarks from the SPEC OMP 2012 suite featuring hand-optimized OpenMP CPU parallelism showcases the simplicity of porting host applications to the GPU. For existing parallel loops, we often match the performance of corresponding manually offloaded kernels, with up to 14.36x speedup on the GPU, validating that our GPU First methodology can effectively guide porting efforts of large legacy applications.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Tian, Shilei and Scogland, Tom and Chapman, Barbara and Doerfert, Johannes},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11686 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@misc{reed_torchfx_2022,
	title = {Torch.fx: {Practical} {Program} {Capture} and {Transformation} for {Deep} {Learning} in {Python}},
	shorttitle = {Torch.fx},
	url = {http://arxiv.org/abs/2112.08429},
	doi = {10.48550/arXiv.2112.08429},
	abstract = {Modern deep learning frameworks provide imperative, eager execution programming interfaces embedded in Python to provide a productive development experience. However, deep learning practitioners sometimes need to capture and transform program structure for performance optimization, visualization, analysis, and hardware integration. We study the different designs for program capture and transformation used in deep learning. By designing for typical deep learning use cases rather than long tail ones, it is possible to create a simpler framework for program capture and transformation. We apply this principle in torch.fx, a program capture and transformation library for PyTorch written entirely in Python and optimized for high developer productivity by ML practitioners. We present case studies showing how torch.fx enables workflows previously inaccessible in the PyTorch ecosystem.},
	urldate = {2023-09-08},
	publisher = {arXiv},
	author = {Reed, James K. and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
	month = mar,
	year = {2022},
	note = {arXiv:2112.08429 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{chen_tvm_2018,
	title = {{TVM}: {An} {Automated} {End}-to-{End} {Optimizing} {Compiler} for {Deep} {Learning}},
	shorttitle = {{TVM}},
	url = {http://arxiv.org/abs/1802.04799},
	doi = {10.48550/arXiv.1802.04799},
	abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	month = oct,
	year = {2018},
	note = {arXiv:1802.04799 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@article{fan_synergy_2023,
	title = {{SYnergy}: {Fine}-grained {Energy}-{Efficient} {Heterogeneous} {Computing} for {Scalable} {Energy} {Saving}},
	abstract = {Energy-efficient computing uses power management techniques such as frequency scaling to save energy. Implementing energyefficient techniques on large-scale computing systems is challenging for several reasons. While most modern architectures, including GPUs, are capable of frequency scaling, these features are often not available on large systems. In addition, achieving higher energy savings requires precise energy tuning because not only applications but also different kernels can have different energy characteristics. We propose SYnergy, a novel energy-efficient approach that spans languages, compilers, runtimes, and job schedulers to achieve unprecedented fine-grained energy savings on large-scale heterogeneous clusters. SYnergy defines an extension to the SYCL programming model that allows programmers to define a specific energy goal for each kernel. For example, a kernel can aim to minimize well-known energy metrics such as EDP and ED2P or to achieve predefined energy-performance tradeoffs, such as the best performance with 25\% energy savings. Through compiler integration and a machine learning model, each kernel is statically optimized for the specific target. On large computing systems, a SLURM plugin allows SYnergy to run on all available devices in the cluster, providing scalable energy savings. The methodology is inherently portable and has been evaluated on both NVIDIA and AMD GPUs. Experimental results show unprecedented improvements in energy and energy-related metrics on real-world applications, as well as scalable energy savings on a 64-GPU cluster.},
	language = {en},
	author = {Fan, Kaijie and D’Antonio, Marco and Carpentieri, Lorenzo and Cosenza, Biagio and Ficarelli, Federico and Cesarini, Daniele},
	year = {2023},
}

@inproceedings{li_hyperscale_2023,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2023},
	title = {Hyperscale {Hardware} {Optimized} {Neural} {Architecture} {Search}},
	isbn = {978-1-4503-9918-0},
	url = {https://dl.acm.org/doi/10.1145/3582016.3582049},
	doi = {10.1145/3582016.3582049},
	abstract = {Recent advances in machine learning have leveraged dramatic increases in computational power, a trend expected to continue in the future. This paper introduces the first Hyperscale Hardware Optimized Neural Architecture Search (H2O-NAS) to automatically design accurate and performant machine learning models tailored to the underlying hardware architecture. H2O-NAS consists of three key components: a new massively parallel “one-shot” search algorithm with intelligent weight sharing, which can scale to search spaces of O(10280) and handle large volumes of production traffic; hardware-optimized search spaces for diverse ML models on heterogeneous hardware; and a novel two-phase hybrid performance model and a multi-objective reward function optimized for large scale deployments. H2O-NAS has been implemented around state-of-the-art machine learning models (e.g. convolutional models, vision transformers, and deep learning recommendation models) and deployed at zettaflop scale in production. Our results demonstrate significant improvements in performance (22\% ∼ 56\%) and energy efficiency (17\% ∼25\%) at same or better quality. Our solution is designed for largescale deployment, streamlining privacy and security processes and reducing manual overhead. This facilitates a smooth and automated transition from research to production.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 3},
	publisher = {Association for Computing Machinery},
	author = {Li, Sheng and Andersen, Garrett and Chen, Tao and Cheng, Liqun and Grady, Julian and Huang, Da and Le, Quoc V. and Li, Andrew and Li, Xin and Li, Yang and Liang, Chen and Lu, Yifeng and Ni, Yun and Pang, Ruoming and Tan, Mingxing and Wicke, Martin and Wu, Gang and Zhu, Shengqi and Ranganathan, Parthasarathy and Jouppi, Norman P.},
	month = mar,
	year = {2023},
	keywords = {Accelerator, Deep Learning, GPU, Hyperscale Hardware, Machine Learning, Neural Architecture Search, Pareto Optimization, TPU},
	pages = {343--358},
}

@inproceedings{ismayilov_multi-gpu_2023,
	address = {New York, NY, USA},
	series = {{ICS} '23},
	title = {Multi-{GPU} {Communication} {Schemes} for {Iterative} {Solvers}: {When} {CPUs} are {Not} in {Charge}},
	isbn = {9798400700569},
	shorttitle = {Multi-{GPU} {Communication} {Schemes} for {Iterative} {Solvers}},
	url = {https://dl.acm.org/doi/10.1145/3577193.3593713},
	doi = {10.1145/3577193.3593713},
	abstract = {This paper proposes a fully autonomous execution model for multi-GPU applications that completely excludes the involvement of the CPU beyond the initial kernel launch. In a typical multi-GPU application, the host serves as the orchestrator of execution by directly launching kernels, issuing communication calls, and acting as a synchronizer for devices. We argue that this orchestration, or control flow path, causes undue overhead and can be delegated entirely to devices to improve performance in applications that require communication among peers. For the proposed CPU-free execution model, we leverage existing techniques such as persistent kernels, thread block specialization, device-side barriers, and device-initiated communication routines to write fully autonomous multi-GPU code and achieve significantly reduced communication overheads. We demonstrate our proposed model on two broadly used iterative solvers, 2D/3D Jacobi stencil and Conjugate Gradient(CG). Compared to the CPU-controlled baselines, the CPU-free model can improve 3D stencil communication latency by 58.8\% and provide a 1.63x speedup for CG on 8 NVIDIA A100 GPUs. The project code is available at https://github.com/ParCoreLab/CPU-Free-model.},
	urldate = {2023-09-05},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Ismayilov, Ismayil and Baydamirli, Javid and Sağbili, Doğan and Wahib, Mohamed and Unat, Didem},
	month = jun,
	year = {2023},
	keywords = {GPU-initiated communication, NVSHMEM, iterative solvers, multi-GPU, persistent kernels},
	pages = {192--202},
}

@misc{yadav_spdistal_2022,
	title = {{SpDISTAL}: {Compiling} {Distributed} {Sparse} {Tensor} {Computations}},
	shorttitle = {{SpDISTAL}},
	url = {http://arxiv.org/abs/2207.13901},
	doi = {10.48550/arXiv.2207.13901},
	abstract = {We introduce SpDISTAL, a compiler for sparse tensor algebra that targets distributed systems. SpDISTAL combines separate descriptions of tensor algebra expressions, sparse data structures, data distribution, and computation distribution. Thus, it enables distributed execution of sparse tensor algebra expressions with a wide variety of sparse data structures and data distributions. SpDISTAL is implemented as a C++ library that targets a distributed task-based runtime system and can generate code for nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates distributed code that achieves performance competitive with hand-written distributed functions for specific sparse tensor algebra expressions and that outperforms general interpretation-based systems by one to two orders of magnitude.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Yadav, Rohan and Aiken, Alex and Kjolstad, Fredrik},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13901 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Programming Languages},
}

@inproceedings{anderson_efficient_2021,
	address = {New York, NY, USA},
	series = {{SPAA} '21},
	title = {Efficient {Parallel} {Self}-{Adjusting} {Computation}},
	isbn = {978-1-4503-8070-6},
	url = {https://dl.acm.org/doi/10.1145/3409964.3461799},
	doi = {10.1145/3409964.3461799},
	abstract = {Self-adjusting computation is an approach for automatically producing dynamic algorithms from static ones. It works by tracking control and data dependencies, and propagating changes through the dependencies when making an update. Extensively studied in the sequential setting, some results on parallel self-adjusting computation exist, but are only applicable to limited classes of computations, or are ad-hoc systems with no theoretical analysis of their performance. In this paper, we present the first system for parallel self-adjusting computation that applies to a wide class of nested parallel algorithms and provides theoretical bounds on the work and span of the resulting dynamic algorithms. Our bounds relate a "distance" measure between computations on different inputs to the cost of propagating an update. The main innovation in the paper is in using Series-Parallel trees (SP trees) to track sequential and parallel control dependencies to allow change propagation to be applied safely in parallel. We demonstrate several example applications, including algorithms for dynamic sequences and dynamic trees. Lastly, we show experimentally that our system allows algorithms to produce updated results over large datasets significantly faster than from-scratch execution, saving both work and parallel time.},
	urldate = {2023-09-04},
	booktitle = {Proceedings of the 33rd {ACM} {Symposium} on {Parallelism} in {Algorithms} and {Architectures}},
	publisher = {Association for Computing Machinery},
	author = {Anderson, Daniel and Blelloch, Guy E. and Baweja, Anubhav and Acar, Umut A.},
	month = jul,
	year = {2021},
	keywords = {dynamic algorithms, incremental computation, parallel algorithms, self-adjusting computation},
	pages = {59--70},
}

@inproceedings{gonthier_memory-aware_2022,
	title = {Memory-{Aware} {Scheduling} of {Tasks} {Sharing} {Data} on {Multiple} {GPUs} with {Dynamic} {Runtime} {Systems}},
	doi = {10.1109/IPDPS53621.2022.00073},
	abstract = {The use of accelerators such as GPUs has become mainstream to achieve high performance on modern computing systems. GPUs come with their own (limited) memory and are connected to the main memory of the machine through a bus (with limited bandwidth). When a computation is started on a GPU, the corresponding data needs to be transferred to the GPU before the computation starts. Such data movements may become a bottleneck for performance, especially when several GPUs have to share the communication bus. Task-based runtime schedulers have emerged as a convenient and efficient way to use such heterogeneous platforms. When processing an application, the scheduler has the knowledge of all tasks available for processing on a GPU, as well as their input data dependencies. Hence, it is able to choose which task to allocate to which GPU and to reorder tasks so as to minimize data movements. We focus on this problem of partitioning and ordering tasks that share some of their input data. We present a novel dynamic strategy based on data selection to efficiently allocate tasks to GPUs and a custom eviction policy, and compare them to existing strategies using either a well-known graph partitioner or standard scheduling techniques in runtime systems. We also improved an offline scheduler recently proposed for a single GPU, by adding load balancing and task stealing capabilities. All strategies have been implemented on top of the STARPU runtime, and we show that our dynamic strategy achieves better performance when scheduling tasks on multiple GPU s with limited memory.},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Gonthier, Maxime and Marchal, Loris and Thibault, Samuel},
	month = may,
	year = {2022},
	note = {ISSN: 1530-2075},
	keywords = {Bandwidth, Distributed databases, Distributed processing, Dynamic scheduling, Eviction policy, Graphics processing units, Load management, Memory-aware scheduling, Runtime, Runtime systems, Tasks sharing data, to read},
	pages = {694--704},
}

@article{zhang_understanding_2022,
	title = {Understanding {GNN} {Computational} {Graph}: {A} {Coordinated} {Computation}, {IO}, and {Memory} {Perspective}},
	volume = {4},
	shorttitle = {Understanding {GNN} {Computational} {Graph}},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/hash/b559156047e50cf316207249d0b5a6c5-Abstract.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Zhang, Hengrui and Yu, Zhongming and Dai, Guohao and Huang, Guyue and Ding, Yufei and Xie, Yuan and Wang, Yu},
	month = apr,
	year = {2022},
	pages = {467--484},
}

@article{xie_synthesizing_2022,
	title = {Synthesizing {Optimal} {Parallelism} {Placement} and {Reduction} {Strategies} on {Hierarchical} {Systems} for {Deep} {Learning}},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/hash/f0f9e98bc2e2f0abc3e315eaa0d808fc-Abstract.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Xie, Ningning and Norman, Tamara and Grewe, Dominik and Vytiniotis, Dimitrios},
	month = apr,
	year = {2022},
	pages = {548--566},
}

@article{hu_giph_2023,
	title = {{GiPH}: {Generalizable} {Placement} {Learning} for {Adaptive} {Heterogeneous} {Computing}},
	volume = {5},
	shorttitle = {{GiPH}},
	url = {https://proceedings.mlsys.org/paper_files/paper/2023/hash/6539a490f171bd2ff910816dff2dde85-Abstract-mlsys2023.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Hu, Yi and Zhang, Chaoran and Andert, Edward and Singh, Harshul and Shrivastava, Aviral and Laudon, James and Zhou, Yanqi and Iannucci, Bob and Joe-Wong, Carlee},
	month = mar,
	year = {2023},
}

@article{zhuang_optimizing_2023,
	title = {On {Optimizing} the {Communication} of {Model} {Parallelism}},
	volume = {5},
	url = {https://proceedings.mlsys.org/paper_files/paper/2023/hash/d0b9a3081f811b2a307c38ad457a487c-Abstract-mlsys2023.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Zhuang, Yonghao and Zheng, Lianmin and Li, Zhuohan and Xing, Eric and Ho, Qirong and Gonzalez, Joseph and Stoica, Ion and Zhang, Hao and Zhao, Hexu},
	month = mar,
	year = {2023},
}

@article{he_transcending_2023,
	title = {Transcending {Runtime}-{Memory} {Tradeoffs} in {Checkpointing} by being {Fusion} {Aware}},
	volume = {5},
	url = {https://proceedings.mlsys.org/paper_files/paper/2023/hash/c443e9d9fc984cda1c5cc447fe2c724d-Abstract-mlsys2023.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {He, Horace and Yu, Shangdi},
	month = mar,
	year = {2023},
}

@inproceedings{zhou_transferable_2020,
	title = {Transferable {Graph} {Optimizers} for {ML} {Compilers}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/9f29450d2eb58feb555078bdefe28aa5-Abstract.html},
	abstract = {Most compilers for machine learning (ML) frameworks need to solve many correlated optimization problems to generate efficient machine code. Current ML compilers rely on heuristics based algorithms to solve these optimization problems one at a time. However, this approach is not only hard to maintain but often leads to sub-optimal solutions especially for newer model architectures. Existing learning based approaches in the literature are sample inefficient, tackle a single optimization problem, and do not generalize to unseen graphs making them infeasible to be deployed in practice. To address these limitations, we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO), based on a scalable sequential attention mechanism over an inductive graph neural network. GO generates decisions on the entire graph rather than on each individual node autoregressively, drastically speeding up the search compared to prior methods. Moreover, we propose recurrent attention layers to jointly optimize dependent graph optimization tasks and demonstrate 33\%-60\% speedup on three graph optimization tasks compared to TensorFlow default optimization. On a diverse set of representative graphs consisting of up to 80,000 nodes, including Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21\% improvement over human experts and 18\% improvement over the prior state of the art with 15x faster convergence, on a device placement task evaluated in real systems.},
	urldate = {2023-09-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhou, Yanqi and Roy, Sudip and Abdolrashidi, Amirali and Wong, Daniel and Ma, Peter and Xu, Qiumin and Liu, Hanxiao and Phothilimtha, Phitchaya and Wang, Shen and Goldie, Anna and Mirhoseini, Azalia and Laudon, James},
	year = {2020},
	pages = {13844--13855},
}

@article{kaufman_learned_2021,
	title = {A {Learned} {Performance} {Model} for {Tensor} {Processing} {Units}},
	volume = {3},
	url = {https://proceedings.mlsys.org/paper_files/paper/2021/hash/6bcfac823d40046dca25ef6d6d59cc3f-Abstract.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Kaufman, Sam and Phothilimthana, Phitchaya and Zhou, Yanqi and Mendis, Charith and Roy, Sudip and Sabne, Amit and Burrows, Mike},
	month = mar,
	year = {2021},
	pages = {387--400},
}

@article{xie_transferable_2022,
	title = {A {Transferable} {Approach} for {Partitioning} {Machine} {Learning} {Models} on {Multi}-{Chip}-{Modules}},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper_files/paper/2022/hash/74e22712c9b50a9b43b2ae54e225888e-Abstract.html},
	language = {en},
	urldate = {2023-09-01},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Xie, Xinfeng and Prabhu, Prakash and Beaugnon, Ulysse and Phothilimthana, Phitchaya and Roy, Sudip and Mirhoseini, Azalia and Brevdo, Eugene and Laudon, James and Zhou, Yanqi},
	month = apr,
	year = {2022},
	pages = {370--381},
}

@misc{phothilimthana_tpugraphs_2023,
	title = {{TpuGraphs}: {A} {Performance} {Prediction} {Dataset} on {Large} {Tensor} {Computational} {Graphs}},
	shorttitle = {{TpuGraphs}},
	url = {http://arxiv.org/abs/2308.13490},
	doi = {10.48550/arXiv.2308.13490},
	abstract = {Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10-20\% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source machine learning programs, featuring popular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, and Transformer. TpuGraphs provides 25x more graphs than the largest graph property prediction dataset (with comparable graph sizes), and 770x larger graphs on average compared to existing performance prediction datasets on machine learning programs. This graph-level prediction task on large graphs introduces new challenges in learning, ranging from scalability, training efficiency, to model quality.},
	urldate = {2023-09-01},
	publisher = {arXiv},
	author = {Phothilimthana, Phitchaya Mangpo and Abu-El-Haija, Sami and Cao, Kaidi and Fatemi, Bahare and Mendis, Charith and Perozzi, Bryan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.13490 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@inproceedings{ibrahim_analyzing_2020,
	address = {New York, NY, USA},
	series = {{PACT} '20},
	title = {Analyzing and {Leveraging} {Shared} {L1} {Caches} in {GPUs}},
	isbn = {978-1-4503-8075-1},
	url = {https://dl.acm.org/doi/10.1145/3410463.3414623},
	doi = {10.1145/3410463.3414623},
	abstract = {Graphics Processing Units (GPUs) concurrently execute thousands of threads, which makes them effective for achieving high throughput for a wide range of applications. However, the memory wall often limits peak throughput. GPUs use caches to address this limitation, and hence several prior works have focused on improving cache hit rates, which in turn can improve throughput for memory-intensive applications. However, almost all of the prior works assume a conventional cache hierarchy where each GPU core has a private local L1 cache and all cores share the L2 cache. Our analysis shows that this canonical organization does not allow optimal utilization of caches because the private nature of L1 caches allows multiple copies of the same cache line to get replicated across cores. We introduce a new shared L1 cache organization, where all cores collectively cache a single copy of the data at only one location (core), leading to zero data replication. We achieve this by allowing each core to cache only a non-overlapping slice of the entire address range. Such a design is useful for significantly improving the collective L1 hit rates but incurs latency overheads from additional communications when a core requests data not allowed to be present in its own cache. While many workloads can tolerate this additional latency, several workloads show performance sensitivities. Therefore, we develop lightweight communication optimization techniques and a run-time mechanism that considers the latency-tolerance characteristics of applications to decide which applications should execute in private versus shared L1 cache organization and reconfigures the caches accordingly. In effect, we achieve significant performance and energy efficiency improvements, at a modest hardware cost, for applications that prefer the shared organization, with little to no impact on other applications.},
	urldate = {2023-08-29},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Ibrahim, Mohamed Assem and Kayiran, Onur and Eckert, Yasuko and Loh, Gabriel H. and Jog, Adwait},
	month = sep,
	year = {2020},
	keywords = {bandwidth, gpus, locality, to read},
	pages = {161--173},
}

@article{henry_compilation_2021,
	title = {Compilation of sparse array programming models},
	volume = {5},
	url = {https://dl.acm.org/doi/10.1145/3485505},
	doi = {10.1145/3485505},
	abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4× to 53.7× when measured against PyData/Sparse for user-defined functions and between 0.98× and 5.53× when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58× to 70.3×, and (where applicable) performs between 0.96× and 28.9× that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56× and 3.50× compared to that of the hand-optimized SuiteSparse:GraphBLAS library.},
	number = {OOPSLA},
	urldate = {2023-08-30},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
	month = oct,
	year = {2021},
	keywords = {Compilation, Sparse Array Programming, Sparse Arrays},
	pages = {128:1--128:29},
}

@article{noauthor_automated_2023,
	title = {Automated {Mapping} of {Task}-{Based} {Programs} onto {Distributed} and {Heterogeneous} {Machines}},
	abstract = {In a parallel and distributed application, a mapping is a selection of a processor for each computation or task and memories for the data collections that each task accesses. Finding high-performance mappings is challenging, particularly on heterogeneous hardware with multiple choices for processors and memories. We show that fast mappings are sensitive to the machine, application, and input. Porting to a new machine, modifying the application, or using a different input size may necessitate re-tuning the mapping to maintain the best possible performance.},
	language = {en},
	year = {2023},
}

@misc{noauthor_automated_nodate,
	title = {‪{Automated} {Mapping} of {Task}-{Based} {Programs} onto {Distributed} and {Heterogeneous} {Machines}‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=75t_IX8AAAAJ&sortby=pubdate&citation_for_view=75t_IX8AAAAJ:Tyk-4Ss8FVUC},
	abstract = {‪TSFX Teixeira, A Henzinger, R Yadav, A Aiken, 2023‬},
	urldate = {2023-08-30},
}

@inproceedings{yadav_distal_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {{DISTAL}: the distributed tensor algebra compiler},
	isbn = {978-1-4503-9265-5},
	shorttitle = {{DISTAL}},
	url = {https://dl.acm.org/doi/10.1145/3519939.3523437},
	doi = {10.1145/3519939.3523437},
	abstract = {We introduce DISTAL, a compiler for dense tensor algebra that targets modern distributed and heterogeneous systems. DISTAL lets users independently describe how tensors and computation map onto target machines through separate format and scheduling languages. The combination of choices for data and computation distribution creates a large design space that includes many algorithms from both the past (e.g., Cannon’s algorithm) and the present (e.g., COSMA). DISTAL compiles a tensor algebra domain specific language to a distributed task-based runtime system and supports nodes with multi-core CPUs and multiple GPUs. Code generated by is competitive with optimized codes for matrix multiply on 256 nodes of the Lassen supercomputer and outperforms existing systems by between 1.8x to 3.7x (with a 45.7x outlier) on higher order tensor operations.},
	urldate = {2023-08-30},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Yadav, Rohan and Aiken, Alex and Kjolstad, Fredrik},
	month = jun,
	year = {2022},
	keywords = {Compilers, Distributed Systems, High Performance Computing},
	pages = {286--300},
}

@inproceedings{sao_scalable_2021,
	address = {New York, NY, USA},
	series = {{HPDC} '21},
	title = {Scalable {All}-pairs {Shortest} {Paths} for {Huge} {Graphs} on {Multi}-{GPU} {Clusters}},
	isbn = {978-1-4503-8217-5},
	url = {https://dl.acm.org/doi/10.1145/3431379.3460651},
	doi = {10.1145/3431379.3460651},
	abstract = {We present an optimized Floyd-Warshall (Floyd-Warshall) algorithm that computes the All-pairs shortest path (APSP) for GPU accelerated clusters. The Floyd-Warshall algorithm due to its structural similarities to matrix-multiplication is well suited for highly parallel GPU architectures. To achieve high parallel efficiency, we address two key algorithmic challenges: reducing high communication overhead and addressing limited GPU memory. To reduce high communication costs, we redesign the parallel (a) to expose more parallelism, (b) aggressively overlap communication and computation with pipelined and asynchronous scheduling of operations, and (c) tailored MPI-collective. To cope with limited GPU memory, we employ an offload model, where the data resides on the host and is transferred to GPU on-demand. The proposed optimizations are supported with detailed performance models for tuning. Our optimized parallel Floyd-Warshall implementation is up to 5x faster than a strong baseline and achieves 8.1 PetaFLOPS/sec on 256{\textasciitilde}nodes of the Summit supercomputer at Oak Ridge National Laboratory. This performance represents 70\% of the theoretical peak and 80\% parallel efficiency. The offload algorithm can handle 2.5x larger graphs with a 20\% increase in overall running time.},
	urldate = {2023-08-30},
	booktitle = {Proceedings of the 30th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {sao, piyush and lu, Hao and Kannan, Ramakrishnan and Thakkar, Vijay and Vuduc, Richard and Potok, Thomas},
	month = jun,
	year = {2021},
	keywords = {all-pair shortest path, distributed algorithm, floyd-warshall algorithm, gpu computing, semi-ring algebra},
	pages = {121--131},
}

@inproceedings{martin-alvarez_configurable_2023,
	title = {Configurable synthetic application for studying malleability in {HPC}},
	doi = {10.1109/PDP59025.2023.00027},
	abstract = {Nowadays, the throughput improvement in large clusters of computers recommends the development of malleable applications. Thus, during the execution of these applications in a job, the resource management system (RMS) can modify its resource allocation, in order to increase the global throughput. There are different alternatives to complete the different steps in which the reallocation of resources is decomposed. To find the best alternatives, this paper introduces a configurable synthetic iterative MPI malleable application capable of modifying, in execution time, the number of MPI processes according to several parameters. The application includes a performance module to measure stages time within steps, from processes management to data redistribution. In this way, the analysis of different scenarios will allow to conclude how the reconfiguration of application has to be made in different circumstances. At the same time, this tool can be used to create workloads that will allow to analyse the impact of malleability on a system and the work in progress.},
	booktitle = {2023 31st {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-{Based} {Processing} ({PDP})},
	author = {Martín-Álvarez, Iker and Aliaga, José I. and Castillo, Maribel and Iserte, Sergio},
	month = mar,
	year = {2023},
	note = {ISSN: 2377-5750},
	keywords = {Computers, Iterative Methods, Iterative methods, MPI, Malleability, Resource management, Simulations, Throughput, Time measurement},
	pages = {128--135},
}

@article{aliaga_survey_2022,
	title = {A {Survey} on {Malleability} {Solutions} for {High}-{Performance} {Distributed} {Computing}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/10/5231},
	doi = {10.3390/app12105231},
	abstract = {Maintaining a high rate of productivity, in terms of completed jobs per unit of time, in High-Performance Computing (HPC) facilities is a cornerstone in the next generation of exascale supercomputers. Process malleability is presented as a straightforward mechanism to address that issue. Nowadays, the vast majority of HPC facilities are intended for distributed-memory applications based on the Message Passing (MP) paradigm. For this reason, many efforts are based on the Message Passing Interface (MPI), the de facto standard programming model. Malleability aims to rescale executions on-the-fly, in other words, reconfigure the number and layout of processes in running applications. Process malleability involves resources reallocation within the HPC system, handling processes of the application, and redistributing data among those processes to resume the execution. This manuscript compiles how different frameworks address process malleability, their main features, their integration in resource management systems, and how they may be used in user codes. This paper is a detailed state-of-the-art devised as an entry point for researchers who are interested in process malleability.},
	language = {en},
	number = {10},
	urldate = {2023-08-30},
	journal = {Applied Sciences},
	author = {Aliaga, Jose I. and Castillo, Maribel and Iserte, Sergio and Martín-Álvarez, Iker and Mayo, Rafael},
	month = jan,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {MPI, adaptive workloads, data redistribution, exascale, job reconfiguration, resource management},
	pages = {5231},
}

@article{bez_io_2023,
	title = {I/{O} {Access} {Patterns} in {HPC} {Applications}: {A} 360-{Degree} {Survey}},
	issn = {0360-0300},
	shorttitle = {I/{O} {Access} {Patterns} in {HPC} {Applications}},
	url = {https://dl.acm.org/doi/10.1145/3611007},
	doi = {10.1145/3611007},
	abstract = {The high-performance computing (HPC) I/O stack has been complex due to multiple software layers, the inter-dependencies among these layers, and the different performance tuning options for each layer. In this complex stack, the definition of an “I/O access pattern” has been re-appropriated to describe what an application is doing to write or read data from the perspective of different layers of the stack, often comprising a different set of features. It has become common having to redefine what is meant when discussing a pattern in every new study as no assumption can be made. This survey aims to propose a baseline taxonomy, harnessing the I/O community’s knowledge over the last 20 years. This definition can serve as a common ground for HPC I/O researchers and developers to apply known I/O tuning strategies and design new strategies for improving I/O performance. We seek to summarize and bring a consensus with the multiple ways to describe a pattern based on common features already used by the community over the years.},
	urldate = {2023-08-30},
	journal = {ACM Computing Surveys},
	author = {Bez, Jean Luca and Byna, Suren and Ibrahim, Shadi},
	month = jul,
	year = {2023},
	note = {Just Accepted},
	keywords = {HPC I/O, I/O access pattern, I/O characterization, storage},
}

@inproceedings{doerfert_breaking_2023,
	address = {New York, NY, USA},
	series = {{PACT} '22},
	title = {Breaking the {Vendor} {Lock}: {Performance} {Portable} {Programming} through {OpenMP} as {Target} {Independent} {Runtime} {Layer}},
	isbn = {978-1-4503-9868-8},
	shorttitle = {Breaking the {Vendor} {Lock}},
	url = {https://dl.acm.org/doi/10.1145/3559009.3569687},
	doi = {10.1145/3559009.3569687},
	abstract = {High performance computing (HPC) systems pervasively feature GPU accelerators. For maximum efficiency, these are usually programmed using vendor-specific languages, such as CUDA. However, this is not portable and leads to vendor lock-in. Existing portable proramming models require transcribing the whole application, which is tedious and often results in sub-optimal performance without necessarily avoiding the need to maintain multiple versions. Although solutions for automated translation exist, they sacrifice either features of the original model, performance, or both. We propose a novel compiler-based approach for performance portable programming of GPUs by generating portable code from the original, vendor-specific application source. Specifically, we present LLVM/Clang extensions for performance portable CUDA by leveraging the existing LLVM/OpenMP offloading infrastructure for portable execution on different GPU architectures. Our contributions include: re-designing the compiler driver for portable toolchain generation, defining a target independent math library, and re-architecting compiler lowering from CUDA APIs to existing and new OpenMP runtime calls. We evaluate our approach using six established CUDA proxy and benchmark applications first on NVIDIA GPUs, to measure the overhead of our portability layer, then secondly on AMD GPUs, to determine the efficacy of our approach. In both experiments we compare the performance to native program versions, i.e., CUDA and HIP. Our approach has minimal overhead compared to non-portable alternatives, thus providing viable performance portability for existing code without cost to the user. We further show CUDA code debugged directly on the host.},
	urldate = {2023-08-29},
	booktitle = {Proceedings of the {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Doerfert, Johannes and Jasper, Marc and Huber, Joseph and Abdelaal, Khaled and Georgakoudis, Giorgis and Scogland, Thomas and Parasyris, Konstantinos},
	year = {2023},
	keywords = {AMDGPU, CUDA, GPGPU, LLVM, OpenMP, performance portability},
	pages = {494--504},
}

@inproceedings{liu_combining_2023,
	address = {New York, NY, USA},
	series = {{PACT} '22},
	title = {Combining {Run}-{Time} {Checks} and {Compile}-{Time} {Analysis} to {Improve} {Control} {Flow} {Auto}-{Vectorization}},
	isbn = {978-1-4503-9868-8},
	url = {https://dl.acm.org/doi/10.1145/3559009.3569663},
	doi = {10.1145/3559009.3569663},
	abstract = {SIMD (Single Instruction Multiple Data) instructions apply the same operation to multiple elements simultaneously. Compilers transform codes to exploit SIMD instructions through auto-vectorization. Control flow can lead to challenges for auto-vectorization tools because compilers conservatively assume branches are divergent. However, it is common that all SIMD lanes follow the same control-path at run-time, a property we call dynamic uniformity. In this paper, we present VecRC (an auto-vectorizer with run-time checks), a novel compile-time technique that uses run-time checks to test for dynamically uniform control flows. Under the assumption of dynamic uniformity, we perform several compile-time analyses that improve control flow auto-vectorization vs state-of-the-art approaches. VecRC leverages dynamic uniformity to vectorize loops with control-dependent loop-carried dependences. Existing strategies use speculation to optimistically execute vector code, and must correct any incorrect computation due to violated run-time assumptions. VecRC performs compile-time analysis based on uniformity to support such dependences without the overhead of speculation. We propose a probability-based cost model to predict the profitability of run-time checks to eliminate the need for specialized profiling or expensive auto-tuning required in existing methods. VecRC is evaluated in LLVM on a diverse range of benchmarks including SPEC2017, NPB, Parboil, TSVC, and Rodinia on Intel Skylake and IBM Power 9 architectures. On the Skylake architecture, geometric mean speedups of 1.31x, 1.20x, 1.19x, and 1.06x over Region Vectorizer, GCC, Clang, and ICC are obtained with VecRC on real benchmark code.},
	urldate = {2023-08-29},
	booktitle = {Proceedings of the {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Bangtian and Laird, Avery and Tsang, Wai Hung and Mahjour, Bardia and Dehnavi, Maryam Mehri},
	year = {2023},
	keywords = {SIMD, compiler optimization, control flow, data dependence, dynamic uniformity, vectorization},
	pages = {439--450},
}

@inproceedings{jangda_model-based_2020,
	address = {New York, NY, USA},
	series = {{PACT} '20},
	title = {Model-{Based} {Warp} {Overlapped} {Tiling} for {Image} {Processing} {Programs} on {GPUs}},
	isbn = {978-1-4503-8075-1},
	url = {https://dl.acm.org/doi/10.1145/3410463.3414649},
	doi = {10.1145/3410463.3414649},
	abstract = {Domain-specific languages that execute image processing pipelines on GPUs, such as Halide and Forma, operate by 1){\textasciitilde}dividing the image into overlapped tiles, and 2){\textasciitilde}fusing loops to improve memory locality. However, current approaches have limitations: 1){\textasciitilde}they require intra thread block synchronization, which has a nontrivial cost, 2){\textasciitilde}they must choose between small tiles that require more overlapped computations or large tiles that increase shared memory access (and lowers occupancy), and 3) their autoscheduling algorithms use simplified GPU models that can result in inefficient global memory accesses. We present a new approach for executing image processing pipelines on GPUs that addresses these limitations as follows. 1) We fuse loops to form overlapped tiles that fit in a single warp, which allows us to use lightweight warp synchronization. 2) We introduce hybrid tiling, which stores overlapped regions in a combination of thread-local registers and shared memory. Thus hybrid tiling either increases occupancy by decreasing shared memory usage or decreases overlapping computations using larger tiles. 3) We present an automatic loop fusion algorithm that considers several factors that affect the performance of GPU kernels. We implement these techniques in PolyMage-GPU, which is a new GPU backend for PolyMage. Our approach produces code that is faster than Halide's manual schedules: 1.65x faster on an NVIDIA GTX 1080Ti and 1.33x faster on an NVIDIA Tesla V100.},
	urldate = {2023-08-29},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Jangda, Abhinav and Guha, Arjun},
	month = sep,
	year = {2020},
	keywords = {graphics processing units, image processing pipelines, polyhedral optimizations},
	pages = {317--328},
}

@inproceedings{galarza_parallelizing_2023,
	address = {Naples, Italy},
	title = {Parallelizing {Multipacting} {Simulation} for the {Design} of {Particle} {Accelerator} {Components}},
	isbn = {9798350337631},
	url = {https://ieeexplore.ieee.org/document/10137163/},
	doi = {10.1109/PDP59025.2023.00030},
	urldate = {2023-08-18},
	booktitle = {2023 31st {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-{Based} {Processing} ({PDP})},
	publisher = {IEEE},
	author = {Galarza, J. and Navaridas, J. and Pascual, Ja. and Romero, T. and Muñoz, Jl. and Bustinduy, I.},
	month = mar,
	year = {2023},
	keywords = {to read},
	pages = {149--153},
}

@inproceedings{nichols_resource_2022,
	title = {Resource {Utilization} {Aware} {Job} {Scheduling} to {Mitigate} {Performance} {Variability}},
	doi = {10.1109/IPDPS53621.2022.00040},
	abstract = {Resource contention on high performance computing (HPC) platforms can lead to significant variation in application performance. When several jobs experience such large variations in run times, it can lead to less efficient use of system resources. It can also lead to users over-estimating their job's expected run time, which degrades the efficiency of the system scheduler. Mitigating performance variation on HPC platforms benefits end users and also enables more efficient use of system resources. In this paper, we present a pipeline for collecting and analyzing system and application performance data for jobs submitted over long periods of time. We use a set of machine learning (ML) models trained on this data to classify performance variation using current system counters. Additionally, we present a new resource-aware job scheduling algorithm that utilizes the ML pipeline and current system state to mitigate job variation. We evaluate our pipeline, ML models, and scheduler using various proxy applications and an actual implementation of the scheduler on an Infiniband-based fat-tree cluster.},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Nichols, Daniel and Marathe, Aniruddha and Shoga, Kathleen and Gamblin, Todd and Bhatele, Abhinav},
	month = may,
	year = {2022},
	note = {ISSN: 1530-2075},
	keywords = {Distributed processing, High performance computing, Machine learning, Measurement, Pipelines, Schedules, Scheduling algorithms, data analytics, machine learning, performance variability, prediction models, scheduling},
	pages = {335--345},
}

@inproceedings{fan_deep_2021,
	title = {Deep {Reinforcement} {Agent} for {Scheduling} in {HPC}},
	doi = {10.1109/IPDPS49936.2021.00090},
	abstract = {Cluster scheduler is crucial in high-performance computing (HPC). It determines when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with specific HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a novel, hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backfilling. A unique training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a specific scheduling objective given by system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. The experiments with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 45\%.},
	booktitle = {2021 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Fan, Yuping and Lan, Zhiling and Childers, Taylor and Rich, Paul and Allcock, William and Papka, Michael E.},
	month = may,
	year = {2021},
	note = {ISSN: 1530-2075},
	keywords = {Distributed processing, Grasping, Neural networks, Processor scheduling, Production, Reinforcement learning, Training, backfilling, cluster scheduling, deep reinforcement learning, high-performance computing, job starvation, resource reservation},
	pages = {807--816},
}

@inproceedings{ravi_evaluating_2023,
	title = {Evaluating {Asynchronous} {Parallel} {I}/{O} on {HPC} {Systems}},
	doi = {10.1109/IPDPS54959.2023.00030},
	abstract = {Parallel I/O is an effective method to optimize data movement between memory and storage for many scientific applications. Poor performance of traditional disk-based file systems has led to the design of I/O libraries which take advantage of faster memory layers, such as on-node memory, present in high-performance computing (HPC) systems. By allowing caching and prefetching of data for applications alternating computation and I/O phases, a faster memory layer also provides opportunities for hiding the latency of I/O phases by overlapping them with computation phases, a technique called asynchronous I/O. Since asynchronous parallel I/O in HPC systems is still in the initial stages of development, there hasn't been a systematic study of the factors affecting its performance.In this paper, we perform a systematic study of various factors affecting the performance and efficacy of asynchronous I/O, we develop a performance model to estimate the aggregate I/O bandwidth achievable by iterative applications using synchronous and asynchronous I/O based on past observations, and we evaluate the performance of the recently developed asynchronous I/O feature of a parallel I/O library (HDF5) using benchmarks and real-world science applications. Our study covers parallel file systems on two large-scale HPC systems: Summit and Cori, the former with a GPFS storage and the latter with a Lustre parallel file system.},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Ravi, John and Byna, Suren and Koziol, Quincey and Tang, Houjun and Becchi, Michela},
	month = may,
	year = {2023},
	note = {ISSN: 1530-2075},
	keywords = {Aggregates, Asynchronous I/O, Bandwidth, Distributed processing, File systems, High performance computing, Modeling, Parallel I/O, Performance Evaluation, Prefetching, Systematics},
	pages = {211--221},
}

@inproceedings{li_fine-grained_2022,
	title = {A {Fine}-grained {Prefetching} {Scheme} for {DGEMM} {Kernels} on {GPU} with {Auto}-tuning {Compatibility}},
	doi = {10.1109/IPDPS53621.2022.00089},
	abstract = {General Matrix Multiplication (GEMM) is one of the fundamental kernels for scientific and high-performance computing. When optimizing the performance of GEMM on GPU, the matrix is usually partitioned into a hierarchy of tiles to fit the thread hierarchy. In practice, the thread-level parallelism is affected not only by the tiling scheme but also by the resources that each tile consumes, such as registers and local data share memory. This paper presents a fine-grained prefetching scheme that improves the thread-level parallelism by balancing the usage of such resources. The gain and loss on instruction and thread level parallelism are analyzed and a mathematical model is developed to estimate the overall performance gain. Moreover, the proposed scheme is integrated into the open-source tool Tensile to automatically generate assembly and tune a collection of kernels to maximize the performance of DGEMM for a family of problem sizes. Experiments show about 1.10X performance speedup on a wide range of matrix sizes for both single and batched matrix-matrix multiplication.},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Li, Jialin and Ye, Huang and Tian, Shaobo and Li, Xinyuan and Zhang, Jian},
	month = may,
	year = {2022},
	note = {ISSN: 1530-2075},
	keywords = {AMD GCN Architecture, DGEMM, Graphics processing units, High performance computing, Libraries, Mathematical models, Parallel processing, Performance gain, Prefetching, Register, TLP, Workgroup Parallelism},
	pages = {863--874},
}

@inproceedings{fan_hybrid_2022,
	title = {Hybrid {Workload} {Scheduling} on {HPC} {Systems}},
	doi = {10.1109/IPDPS53621.2022.00052},
	abstract = {Traditionally, on-demand, rigid, and malleable applications have been scheduled and executed on separate systems. The ever-growing workload demands and rapidly developing HPC infrastructure trigger the interest of converging these applications on a single HPC system. Although allocating the hybrid workloads within one system could potentially improve system efficiency, it is difficult to balance the tradeoff between the responsiveness of on-demand requests, incentive for malleable jobs, and the performance of rigid applications. In this study, we present several scheduling mechanisms to address the issues involved in co-scheduling on-demand, rigid, and malleable jobs on a single HPC system. We extensively evaluate and compare their performance under various configurations and workloads. Our experimental results show that our proposed mechanisms are capable of serving on-demand workloads with minimal delay, offering incentives for declaring malleability, and improving system performance.},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Fan, Yuping and Lan, Zhiling and Rich, Paul and Allcock, William and Papka, Michael E.},
	month = may,
	year = {2022},
	note = {ISSN: 1530-2075},
	keywords = {Delays, Distributed processing, Production, System performance, cluster scheduling, high-performance computing, malleable jobs, on-demand jobs, rigid jobs},
	pages = {470--480},
}

@inproceedings{moses_high-performance_2023,
	address = {New York, NY, USA},
	series = {{PPoPP} '23},
	title = {High-{Performance} {GPU}-to-{CPU} {Transpilation} and {Optimization} via {High}-{Level} {Parallel} {Constructs}},
	isbn = {9798400700156},
	url = {https://dl.acm.org/doi/10.1145/3572848.3577475},
	doi = {10.1145/3572848.3577475},
	abstract = {While parallelism remains the main source of performance, architectural implementations and programming models change with each new hardware generation, often leading to costly application re-engineering. Most tools for performance portability require manual and costly application porting to yet another programming model. We propose an alternative approach that automatically translates programs written in one programming model (CUDA), into another (CPU threads) based on Polygeist/MLIR. Our approach includes a representation of parallel constructs that allows conventional compiler transformations to apply transparently and without modification and enables parallelism-specific optimizations. We evaluate our framework by transpiling and optimizing the CUDA Rodinia benchmark suite for a multi-core CPU and achieve a 58\% geomean speedup over handwritten OpenMP code. Further, we show how CUDA kernels from PyTorch can efficiently run and scale on the CPU-only Supercomputer Fugaku without user intervention. Our PyTorch compatibility layer making use of transpiled CUDA PyTorch kernels outperforms the PyTorch CPU native backend by 2.7×.},
	urldate = {2023-08-27},
	booktitle = {Proceedings of the 28th {ACM} {SIGPLAN} {Annual} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Moses, William S. and Ivanov, Ivan R. and Domke, Jens and Endo, Toshio and Doerfert, Johannes and Zinenko, Oleksandr},
	month = feb,
	year = {2023},
	keywords = {CUDA, MLIR, barrier synchronization, polygeist},
	pages = {119--134},
}

@inproceedings{tan_what_2020,
	address = {New York, NY, USA},
	series = {{ICS} '20},
	title = {What every scientific programmer should know about compiler optimizations?},
	isbn = {978-1-4503-7983-0},
	url = {https://dl.acm.org/doi/10.1145/3392717.3392754},
	doi = {10.1145/3392717.3392754},
	abstract = {Compilers are an indispensable component in the software stack. Besides generating machine code, compilers perform multiple optimizations to improve code performance. Typically, scientific programmers treat compilers as a blackbox and expect them to optimize code thoroughly. However, optimizing compilers are not performance panacea. They can miss optimization opportunities or even introduce inefficiencies that are not in the source code. There is a lack of tool infrastructures and datasets that can provide such a study to help understand compiler optimizations. In this paper, we investigate an important compiler optimization---dead and redundant operation elimination. We first develop a tool CIDetector to analyze a large number of programs. In our analysis, we select 12 representative programs from different domains to form a dataset called CIBench. We utilize five compilers to optimize CIBench with the highest optimization options available and leverage CIDetector to study each generated binary. We provide insights into two aspects. First, we show that modern compilers miss several optimization opportunities, in fact they even introduce some inefficiencies, which require programmers to refactor the source code. Second, we show how compilers have advanced in a vertical evolution (the same compiler of different release versions) and a horizontal comparison (different compilers of the most recent releases). With empirical studies, we provide insights for software engineers, compiler writers, and tool developers.},
	urldate = {2023-08-24},
	booktitle = {Proceedings of the 34th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Jialiang and Jiao, Shuyin and Chabbi, Milind and Liu, Xu},
	month = jun,
	year = {2020},
	keywords = {binary analysis, compiler inefficiencies, profiling, redundancy},
	pages = {1--12},
}

@inproceedings{wang_codeseer_2020,
	address = {New York, NY, USA},
	series = {{ICS} '20},
	title = {{CodeSeer}: input-dependent code variants selection via machine learning},
	isbn = {978-1-4503-7983-0},
	shorttitle = {{CodeSeer}},
	url = {https://dl.acm.org/doi/10.1145/3392717.3392741},
	doi = {10.1145/3392717.3392741},
	abstract = {In high performance computing (HPC), scientific simulation codes are executed repeatedly with different inputs. The peak performance of these programs heavily depends on various compiler optimizations, which are often selected agnostically on program input or may be selected with sensitivity to just a single input. When subsequently executed, often with different inputs, performance may suffer for all or all but the one input tested, and for the latter potentially even compared to the O3 baseline. This work proposes a new auto-tuning framework, CodeSeer, to assess and improve existing input-agnostic or single-input centric rigid application tuning methods. Aided by CodeSeer, it is observed that modern HPC programs expose different types of input sensitivities, which present a significant challenge for prior work. To tackle this problem, CodeSeer proceeds with several machine learning models to predict the best per-input code variant on-the-fly. Our evaluation shows that CodeSeer incurs less than 0.01 second overhead, predicts the best code variant with a geometric mean precision 92\% of the time and is capable of improving per-input peak performance to unprecedented levels.},
	urldate = {2023-08-24},
	booktitle = {Proceedings of the 34th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Tao and Jain, Nikhil and Boehme, David and Beckingsale, David and Mueller, Frank and Gamblin, Todd},
	month = jun,
	year = {2020},
	keywords = {HPC, NP-complete, OpenMP, auto-tuning, code variant, iterative compilation, machine-learning},
	pages = {1--11},
}

@inproceedings{boemer_intel_2021,
	address = {Virtual Event Republic of Korea},
	title = {Intel {HEXL}: {Accelerating} {Homomorphic} {Encryption} with {Intel} {AVX512}-{IFMA52}},
	isbn = {978-1-4503-8656-2},
	shorttitle = {Intel {HEXL}},
	url = {https://dl.acm.org/doi/10.1145/3474366.3486926},
	doi = {10.1145/3474366.3486926},
	abstract = {Modern implementations of homomorphic encryption (HE) rely heavily on polynomial arithmetic over a finite field. This is particularly true of the BGV, BFV, and CKKS HE schemes. Two of the biggest performance bottlenecks in HE primitives and applications are polynomial modular multiplication and the forward and inverse number-theoretic transform (NTT). Here, we introduce Intel® Homomorphic Encryption Acceleration Library (Intel® HEXL), a C++ library which provides optimized implementations of polynomial arithmetic for Intel® processors. Intel HEXL takes advantage of the recent Intel® Advanced Vector Extensions 512 (Intel® AVX512) instruction set to provide state-of-the-art implementations of the NTT and modular multiplication, measuring up to 7.2x single-threaded speedup over a native C++ baseline. Intel HEXL is available open-source at https://github.com/intel/hexl under the Apache 2.0 license and has been adopted by the Microsoft SEAL and PALISADE homomorphic encryption libraries.},
	language = {en},
	urldate = {2023-08-23},
	booktitle = {Proceedings of the 9th on {Workshop} on {Encrypted} {Computing} \& {Applied} {Homomorphic} {Cryptography}},
	publisher = {ACM},
	author = {Boemer, Fabian and Kim, Sejun and Seifu, Gelila and D.M. De Souza, Fillipe and Gopal, Vinodh},
	month = nov,
	year = {2021},
	pages = {57--62},
}

@misc{zhai_accelerating_2021,
	title = {Accelerating {Encrypted} {Computing} on {Intel} {GPUs}},
	url = {http://arxiv.org/abs/2109.14704},
	abstract = {Homomorphic Encryption (HE) is an emerging encryption scheme that allows computations to be performed directly on encrypted messages. This property provides promising applications such as privacy-preserving deep learning and cloud computing. Prior works have been proposed to enable practical privacy-preserving applications with architectural-aware optimizations on CPUs, GPUs and FPGAs. However, there is no systematic optimization for the whole HE pipeline on Intel GPUs. In this paper, we present the ﬁrst-ever SYCL-based GPU backend for Microsoft SEAL APIs. We perform optimizations from instruction level, algorithmic level and application level to accelerate our HE library based on the Cheon, Kim, Kim and Song (CKKS) scheme on Intel GPUs. The performance is validated on two latest Intel GPUs. Experimental results show that our staged optimizations together with optimizations including low-level optimizations and kernel fusion accelerate the Number Theoretic Transform (NTT), a key algorithm for HE, by up to 9.93X compared with the naive GPU baseline. The rooﬂine analysis conﬁrms that our optimized NTT reaches 79.8\% and 85.7\% of the peak performance on two GPU devices. Through the highly optimized NTT and the assembly-level optimization, we obtain 2.32X - 3.05X acceleration for HE evaluation routines. In addition, our all-together systematic optimizations improve the performance of encrypted element-wise polynomial matrix multiplication application by up to 3.10X.},
	language = {en},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Zhai, Yujia and Ibrahim, Mohannad and Qiu, Yiqin and Boemer, Fabian and Chen, Zizhong and Titov, Alexey and Lyashevsky, Alexander},
	month = sep,
	year = {2021},
	note = {arXiv:2109.14704 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Hardware Architecture},
}

@article{lloret-talavera_enabling_2022,
	title = {Enabling {Homomorphically} {Encrypted} {Inference} for {Large} {DNN} {Models}},
	volume = {71},
	issn = {0018-9340, 1557-9956, 2326-3814},
	url = {http://arxiv.org/abs/2103.16139},
	doi = {10.1109/TC.2021.3076123},
	abstract = {The proliferation of machine learning services in the last few years has raised data privacy concerns. Homomorphic encryption (HE) enables inference using encrypted data but it incurs 100x–10,000x memory and runtime overheads. Secure deep neural network (DNN) inference using HE is currently limited by computing and memory resources, with frameworks requiring hundreds of gigabytes of DRAM to evaluate small models. To overcome these limitations, in this paper we explore the feasibility of leveraging hybrid memory systems comprised of DRAM and persistent memory. In particular, we explore the recently-released Intel® Optane™ PMem technology and the Intel® HE-Transformer nGraph® to run large neural networks such as MobileNetV2 (in its largest variant) and ResNet-50 for the ﬁrst time in the literature. We present an in-depth analysis of the efﬁciency of the executions with different hardware and software conﬁgurations. Our results conclude that DNN inference using HE incurs on friendly access patterns for this memory conﬁguration, yielding efﬁcient executions.},
	language = {en},
	number = {5},
	urldate = {2023-08-22},
	journal = {IEEE Transactions on Computers},
	author = {Lloret-Talavera, Guillermo and Jorda, Marc and Servat, Harald and Boemer, Fabian and Chauhan, Chetan and Tomishima, Shigeki and Shah, Nilesh N. and Peña, Antonio J.},
	month = may,
	year = {2022},
	note = {arXiv:2103.16139 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Performance},
	pages = {1145--1155},
}

@article{steiner_value_nodate,
	title = {Value {Learning} for {Throughput} {Optimizationof} {Deep} {Neural} {Networks}},
	abstract = {As the usage of machine learning techniques is becoming ubiquitous, the efﬁcient execution of neural networks is crucial to many applications. Frameworks, such as Halide and TVM, separate the algorithmic representation of the deep learning model from the schedule that determines its implementation. Finding good schedules, however, remains extremely challenging. Auto-tuning methods, which search the space of valid schedules and execute each candidate on the hardware, identify some of the best performing schedules, but the search can take hours, hampering the productivity of deep learning practitioners. What is needed is a method that achieves a similar performance without extensive search, delivering the needed efﬁciency quickly. We model the scheduling process as a sequence of optimization choices, and present a new technique to accurately predict the expected performance of a partial schedule using a LSTM over carefully engineered features that describe each DNN operator and their current scheduling choices. Leveraging these predictions we are able to make these optimization decisions greedily and, without any executions on the target hardware, rapidly identify an efﬁcient schedule.},
	language = {en},
	author = {Steiner, Benoit and Cummins, Chris and He, Horace and Leather, Hugh},
}

@misc{zhang_expediting_2023,
	title = {Expediting {Distributed} {DNN} {Training} with {Device} {Topology}-{Aware} {Graph} {Deployment}},
	url = {http://arxiv.org/abs/2302.06126},
	doi = {10.48550/arXiv.2302.06126},
	abstract = {This paper presents TAG, an automatic system to derive optimized DNN training graph and its deployment onto any device topology, for expedited training in device- and topology- heterogeneous ML clusters. We novelly combine both the DNN computation graph and the device topology graph as input to a graph neural network (GNN), and join the GNN with a search-based method to quickly identify optimized distributed training strategies. To reduce communication in a heterogeneous cluster, we further explore a lossless gradient compression technique and solve a combinatorial optimization problem to automatically apply the technique for training time minimization. We evaluate TAG with various representative DNN models and device topologies, showing that it can achieve up to 4.56x training speed-up as compared to existing schemes. TAG can produce efficient deployment strategies for both unseen DNN models and unseen device topologies, without heavy fine-tuning.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Zhang, Shiwei and Yi, Xiaodong and Diao, Lansong and Wu, Chuan and Wang, Siyu and Lin, Wei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06126 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{fu_towards_2023,
	title = {Towards {Exascale} {Computation} for {Turbomachinery} {Flows}},
	url = {http://arxiv.org/abs/2308.06605},
	doi = {10.48550/arXiv.2308.06605},
	abstract = {A state-of-the-art large eddy simulation code has been developed to solve compressible flows in turbomachinery. The code has been engineered with a high degree of scalability, enabling it to effectively leverage the many-core architecture of the new Sunway system. A consistent performance of 115.8 DP-PFLOPs has been achieved on a high-pressure turbine cascade consisting of over 1.69 billion mesh elements and 865 billion Degree of Freedoms (DOFs). By leveraging a high-order unstructured solver and its portability to large heterogeneous parallel systems, we have progressed towards solving the grand challenge problem outlined by NASA, which involves a time-dependent simulation of a complete engine, incorporating all the aerodynamic and heat transfer components.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Fu, Yuhang and Shen, Weiqi and Cui, Jiahuan and Zheng, Yao and Yang, Guangwen and Liu, Zhao and Zhang, Jifa and Ji, Tingwei and Xie, Fangfang and Lv, Xiaojing and Liu, Hanyue and Liu, Xu and Liu, Xiyang and Song, Xiaoyu and Tao, Guocheng and Yan, Yan and Tucker, Paul and Miller, Steven A. E. and Luo, Shirui and Koric, Seid and Zheng, Weimin},
	month = aug,
	year = {2023},
	note = {arXiv:2308.06605 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@misc{raj_reinforcement_2023,
	title = {A {Reinforcement} {Learning} {Approach} for {Performance}-aware {Reduction} in {Power} {Consumption} of {Data} {Center} {Compute} {Nodes}},
	url = {http://arxiv.org/abs/2308.08069},
	doi = {10.48550/arXiv.2308.08069},
	abstract = {As Exascale computing becomes a reality, the energy needs of compute nodes in cloud data centers will continue to grow. A common approach to reducing this energy demand is to limit the power consumption of hardware components when workloads are experiencing bottlenecks elsewhere in the system. However, designing a resource controller capable of detecting and limiting power consumption on-the-fly is a complex issue and can also adversely impact application performance. In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance. Employing a Proximal Policy Optimization (PPO) agent to learn an optimal policy on a mathematical model of the compute nodes, we demonstrate and evaluate using the STREAM benchmark how a trained agent running on actual hardware can take actions by balancing power consumption and application performance.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Raj, Akhilesh and Perarnau, Swann and Gokhale, Aniruddha},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08069 [cs, eess]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{agrawal_towards_2023,
	title = {Towards {Benchmarking} {Power}-{Performance} {Characteristics} of {Federated} {Learning} {Clients}},
	url = {http://arxiv.org/abs/2308.08270},
	doi = {10.48550/arXiv.2308.08270},
	abstract = {Federated Learning (FL) is a decentralized machine learning approach where local models are trained on distributed clients, allowing privacy-preserving collaboration by sharing model updates instead of raw data. However, the added communication overhead and increased training time caused by heterogenous data distributions results in higher energy consumption and carbon emissions for achieving similar model performance than traditional machine learning. At the same time, efficient usage of available energy is an important requirement for battery constrained devices. Because of this, many different approaches on energy-efficient and carbon-efficient FL scheduling and client selection have been published in recent years. However, most of this research oversimplifies power performance characteristics of clients by assuming that they always require the same amount of energy per processed sample throughout training. This overlooks real-world effects arising from operating devices under different power modes or the side effects of running other workloads in parallel. In this work, we take a first look on the impact of such factors and discuss how better power-performance estimates can improve energy-efficient and carbon-efficient FL scheduling.},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Agrawal, Pratik and Wiesner, Philipp and Kao, Odej},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08270 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
}

@article{dave_hardware_2021,
	title = {Hardware {Acceleration} of {Sparse} and {Irregular} {Tensor} {Computations} of {ML} {Models}: {A} {Survey} and {Insights}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Hardware {Acceleration} of {Sparse} and {Irregular} {Tensor} {Computations} of {ML} {Models}},
	url = {https://ieeexplore.ieee.org/document/9507542/},
	doi = {10.1109/JPROC.2021.3098483},
	language = {en},
	number = {10},
	urldate = {2023-08-18},
	journal = {Proceedings of the IEEE},
	author = {Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
	month = oct,
	year = {2021},
	pages = {1706--1752},
}

@inproceedings{kynigos_novel_2023,
	title = {A {Novel} {Simulation} {Methodology} for {Silicon} {Photonic} {Switching} {Fabrics}},
	doi = {10.1109/ISPASS57527.2023.00020},
	abstract = {Optical communication based on silicon photonics is a promising candidate for future networks. However, a key component that still presents challenges is a practical, silicon photonics-based, high performance switch with a high port count. The impracticality of buffering traffic in the optical domain mandates the use of circuit switching at the transmission level. This renders the photonic power penalty dependent on many factors, including architectural aspects and, most importantly, the switch load. Since the latter changes dynamically with network traffic we argue that simulating silicon photonics-based switches requires considering the photonic power penalty under dynamic workloads, which is not supported by state-of-the-art techniques. In this paper, we show how to simultaneously simulate both the overall switch as well as the photonic power penalty, by proposing a novel combination of the bufferless nature of photonic fabrics, flow-level simulation and optical beam propagation modelling. This approach enables a simulator to consider different kinds of switching fabrics and photonic components. We focus on how to model Beneš photonic switching fabrics formed with Mach-Zehnder Interferometers and consider their deployment as switching cores for top-of-rack switches. We compare our simulation with the published data from two fabricated chips and found accuracy is within 0. 5dB with respect to insertion loss, and within 3dB with respect to crosstalk. As a use-case, we evaluate the impact of routing algorithms on the photonic power penalty and found this can reduce the worst-case photonic power penalty by up to 4dB.},
	booktitle = {2023 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	author = {Kynigos, Markos and Navaridas, Javier and Pascual, Jose and Luján, Mikel},
	month = apr,
	year = {2023},
	keywords = {Control systems, Crosstalk, Optical switches, Performance Analysis, Photonic Switching Fabrics, Routing, Silicon photonics, Simulation, Telecommunication traffic, Wavelength division multiplexing},
	pages = {114--123},
}

@article{belenguer_gowfeda_2022,
	title = {Göwfeda {Novel} {Federated} {Network} {Intrusion} {Detection} {System}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4261807},
	doi = {10.2139/ssrn.4261807},
	abstract = {Network intrusion detection systems are evolving into intelligent systems that perform data analysis while searching for anomalies in their environment. Indeed, the development of deep learning techniques paved the way to build more complex and effective threat detection models. However, training those models may be computationally infeasible in most Edge or IoT devices. Current approaches rely on powerful centralized servers that receive data from all their parties — violating basic privacy constraints and substantially affecting response times and operational costs due to the huge communication overheads. To mitigate these issues, Federated Learning emerged as a promising approach, where different agents collaboratively train a shared model, without exposing training data to others or requiring a compute-intensive centralized infrastructure. This work presents GöwFed, a novel network threat detection system that combines the usage of Gower Dissimilarity matrices and Federated averaging. Different approaches of GöwFed have been developed based on state-of the-art knowledge: (1) a vanilla version — achieving a median point of [0.888, 0.960] in the PR space and a median accuracy of 0.930; and (2) a version instrumented with an attention mechanism — achieving comparable results when 0.8 of the best performing nodes contribute to the model. Furthermore, each variant has been tested using simulation oriented tools provided by TensorFlow Federated framework. In the same way, a centralized analogous development of the Federated systems is carried out to explore their differences in terms of scalability and performance — the median point of the experiments is [0.987, 0.987]) and the median accuracy is 0.989. Overall, GöwFed intends to be the first stepping stone towards the combined usage of Federated Learning and Gower Dissimilarity matrices to detect network threats in industrial-level networks.},
	language = {en},
	urldate = {2023-08-18},
	journal = {SSRN Electronic Journal},
	author = {Belenguer, Aitor and Pascual, Jose A. and Navaridas, Javier},
	year = {2022},
}

@article{menikkumbura_congestion_2023,
	title = {Congestion {Control} for {Datacenter} {Networks}: {A} {Control}-{Theoretic} {Approach}},
	volume = {34},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Congestion {Control} for {Datacenter} {Networks}},
	url = {https://ieeexplore.ieee.org/document/10082870/},
	doi = {10.1109/TPDS.2023.3259799},
	abstract = {In this article, we present RoCC, a robust congestion control approach for datacenter networks based on RDMA. RoCC leverages switch queue size as an input to a PI controller, which computes the fair data rate of ﬂows in the queue. The PI parameters are self-tuning to guarantee stability, rapid convergence, and fair and near-optimal throughput in a wide range of congestion scenarios. Our simulation and DPDK implementation results show that RoCC can achieve up to 7× reduction in PFC frames generated under high load levels, compared to DCQCN. At the same time, RoCC can achieve 1.7 − 4.5× and 1.4 − 3.9× lower tail latency for long ﬂows and 2.1 − 7× and 3.5 − 8.2× lower tail latency for short ﬂows, compared to DCQCN and HPCC, respectively. We also ﬁnd that RoCC does not require PFC. The functional components of RoCC can be efﬁciently implemented in P4 and FPGA-based switch hardware.},
	language = {en},
	number = {5},
	urldate = {2023-08-17},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Menikkumbura, Danushka and Taheri, Parvin and Vanini, Erico and Fahmy, Sonia and Eugster, Patrick and Edsall, Tom},
	month = may,
	year = {2023},
	pages = {1682--1696},
}

@inproceedings{kang_study_2022,
	address = {Dallas, TX, USA},
	title = {Study of {Workload} {Interference} with {Intelligent} {Routing} on {Dragonfly}},
	isbn = {978-1-66545-444-5},
	url = {https://ieeexplore.ieee.org/document/10046126/},
	doi = {10.1109/SC41404.2022.00025},
	abstract = {Dragonfly interconnect is a crucial network technology for supercomputers. To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair. While link utilization is increased, workload performance is often offset by network contention. Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency. However, its effectiveness in reducing workload interference is unknown. In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragonfly system. We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns. We also present two metrics to characterize application communication intensity. Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics. Several key insights are made from the analysis.},
	language = {en},
	urldate = {2023-08-17},
	booktitle = {{SC22}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Kang, Yao and Wang, Xin and Lan, Zhiling},
	month = nov,
	year = {2022},
	pages = {1--14},
}

@inproceedings{khan_vico_2022,
	address = {Virtual Event},
	title = {{VICO}: demand-driven verification for improving compiler optimizations},
	isbn = {978-1-4503-9281-5},
	shorttitle = {{VICO}},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532393},
	doi = {10.1145/3524059.3532393},
	abstract = {In spite of tremendous advances in data dependence and dataflow analysis techniques, state-of-the-art optimizing compilers continue to suffer from imprecisions and miss potential optimization opportunities. These imprecisions result from statically unknown characteristics of variables that participate in the dependence systems, or aliases that affect key safety properties, which must be conservatively assumed. However, with the increased tractability of verification on modern systems, a demand-driven solution to this problem can be envisioned. In this work, we model loop optimization constraints as loop-invariants, with the goal of proving their runtime behaviour under all inputs. Our proposed framework VICO, first detects the unresolved constraints whose conservative assumption negatively affects specific compiler optimizations. These constraints are then modeled as potential invariants and are verified on a demand-driven basis. Finally, VICO incorporates the verified invariants in the analysis, which results in superior optimization. For this purpose, VICO converts conservative constraints identified by the LLVM compiler and parallelization tool PLuTo, to potential invariants, that are further verified by SMACK verification tool. Following such an approach enables us to target numerous optimizations at different compilation phases - automatic parallelization and loop transformations at the source-level, and register allocation, and global value numbering (GVN), at the IR-level. Our results show that VICO improves the precision of dependence analysis by 45\% in real-world cases, leading to superior optimization in over 75 loops in different scenarios like mathematical simulations and solvers. The improvement in dependence precision led to an average speedup of 14.7x on Apple M1 Pro and 6.07x on Intel Xeon E5-2660 systems. In addition, VICO also enhances LLVM’s alias analysis leading to improvements in LLVM backend optimizations and decreased code size by 4\% alongside improved execution time by 2.2\% in numerous linux programs and SPEC benchmarks with (mostly) low verification time.},
	language = {en},
	urldate = {2023-08-17},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Khan, Sharjeel and Chatterjee, Bodhisatwa and Pande, Santosh},
	month = jun,
	year = {2022},
	pages = {1--14},
}

@inproceedings{lorenzon_seamless_2022,
	address = {Virtual Event},
	title = {Seamless optimization of the {GEMM} kernel for task-based programming models},
	isbn = {978-1-4503-9281-5},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532385},
	doi = {10.1145/3524059.3532385},
	abstract = {The general matrix-matrix multiplication (GEMM) kernel is a fundamental building block of many scientific applications. Many libraries such as Intel MKL and BLIS provide highly optimized sequential and parallel versions of this kernel. The parallel implementations of the GEMM kernel rely on the well-known fork-join execution model to exploit multi-core systems efficiently. However, these implementations are not well suited for task-based applications as they break the data-flow execution model. In this paper, we present a task-based implementation of the GEMM kernel that can be seamlessly leveraged by task-based applications while providing better performance than the fork-join version. Our implementation leverages several advanced features of the OmpSs-2 programming model and a new heuristic to select the best parallelization strategy and blocking parameters based on the matrix and hardware characteristics. When evaluating the performance and energy consumption on two modern multi-core systems, we show that our implementations provide significant performance improvements over an optimized OpenMP fork-join implementation, and can beat vendor implementations of the GEMM (e.g., Intel MKL and AMD AOCL). We also demonstrate that a real application can leverage our optimized task-based implementation to enhance performance.},
	language = {en},
	urldate = {2023-08-17},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Lorenzon, Arthur F. and Marques, Sandro M. V. N. and Navarro, Antoni and Beltran, Vicenç},
	month = jun,
	year = {2022},
	pages = {1--11},
}

@inproceedings{bian_online_2021,
	address = {St. Louis Missouri},
	title = {Online evolutionary batch size orchestration for scheduling deep learning workloads in {GPU} clusters},
	isbn = {978-1-4503-8442-1},
	url = {https://dl.acm.org/doi/10.1145/3458817.3480859},
	doi = {10.1145/3458817.3480859},
	abstract = {Efficient GPU resource scheduling is essential to maximize resource utilization and save training costs for the increasing amount of deep learning workloads in shared GPU clusters. Existing GPU schedulers largely rely on static policies to leverage the performance characteristics of deep learning jobs. However, they can hardly reach optimal efficiency due to the lack of elasticity. To address the problem, we propose ONES, an ONline Evolutionary Scheduler for elastic batch size orchestration. ONES automatically manages the elasticity of each job based on the training batch size, so as to maximize GPU utilization and improve scheduling efficiency. It determines the batch size for each job through an online evolutionary search that can continuously optimize the scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on TACC’s Longhorn supercomputers. The results show that ONES can outperform the prior deep learning schedulers with a significantly shorter average job completion time.},
	language = {en},
	urldate = {2023-08-17},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {ACM},
	author = {Bian, Zhengda and Li, Shenggui and Wang, Wei and You, Yang},
	month = nov,
	year = {2021},
	pages = {1--15},
}

@misc{wang_topoopt_2022,
	title = {{TopoOpt}: {Co}-optimizing {Network} {Topology} and {Parallelization} {Strategy} for {Distributed} {Training} {Jobs}},
	shorttitle = {{TopoOpt}},
	url = {http://arxiv.org/abs/2202.00433},
	abstract = {We propose TOPOOPT, a novel direct-connect fabric for deep neural network (DNN) training workloads. TOPOOPT cooptimizes the distributed training process across three dimensions: computation, communication, and network topology. We demonstrate the mutability of AllReduce trafﬁc, and leverage this property to construct efﬁcient network topologies for DNN training jobs. TOPOOPT then uses an alternating optimization technique and a group theory-inspired algorithm called TotientPerms to ﬁnd the best network topology and routing plan, together with a parallelization strategy. We build a fully functional 12-node direct-connect prototype with remote direct memory access (RDMA) forwarding at 100 Gbps. Large-scale simulations on real distributed training models show that, compared to similar-cost Fat-tree interconnects, TOPOOPT reduces DNN training time by up to 3.4×.},
	language = {en},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Wang, Weiyang and Khazraee, Moein and Zhong, Zhizhen and Ghobadi, Manya and Jia, Zhihao and Mudigere, Dheevatsa and Zhang, Ying and Kewitsch, Anthony},
	month = sep,
	year = {2022},
	note = {arXiv:2202.00433 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
}

@misc{ma_powerfusion_2023,
	title = {{PowerFusion}: {A} {Tensor} {Compiler} with {Explicit} {Data} {Movement} {Description} and {Instruction}-level {Graph} {IR}},
	shorttitle = {{PowerFusion}},
	url = {http://arxiv.org/abs/2307.04995},
	abstract = {Deep neural networks (DNNs) are of critical use in different domains. To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domainspecific accelerators. Existing tensor compilers mainly focus on optimizing computation efficiency. However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance. The lack of direct description of memory access and data dependence in current tensor compilers’ intermediate representation (IR) brings significant challenges to generate memory-efficient code.},
	language = {en},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Ma, Zixuan and Wang, Haojie and Xing, Jingze and Zheng, Liyan and Zhang, Chen and Cao, Huanqi and Huang, Kezhao and Tang, Shizhi and Wang, Penghan and Zhai, Jidong},
	month = jul,
	year = {2023},
	note = {arXiv:2307.04995 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_221006640_2022,
	title = {[2210.06640] {Compute}-{Efficient} {Deep} {Learning}: {Algorithmic} {Trends} and {Opportunities}},
	url = {https://arxiv.org/abs/2210.06640},
	urldate = {2022-12-03},
	month = dec,
	year = {2022},
}

@misc{phuong_formal_2022,
	title = {Formal {Algorithms} for {Transformers}},
	url = {http://arxiv.org/abs/2207.09238},
	doi = {10.48550/arXiv.2207.09238},
	abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Phuong, Mary and Hutter, Marcus},
	month = jul,
	year = {2022},
	note = {Issue: arXiv:2207.09238
arXiv:2207.09238 [cs]},
	keywords = {!!!, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{shoeybi_megatron-lm_2020,
	title = {Megatron-{LM}: {Training} {Multi}-{Billion} {Parameter} {Language} {Models} {Using} {Model} {Parallelism}},
	shorttitle = {Megatron-{LM}},
	url = {http://arxiv.org/abs/1909.08053},
	doi = {10.48550/arXiv.1909.08053},
	abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
	urldate = {2023-02-10},
	publisher = {arXiv},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
	month = mar,
	year = {2020},
	note = {Issue: arXiv:1909.08053
arXiv:1909.08053 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{navon_equivariant_2023,
	title = {Equivariant {Architectures} for {Learning} in {Deep} {Weight} {Spaces}},
	url = {http://arxiv.org/abs/2301.12780},
	doi = {10.48550/arXiv.2301.12780},
	abstract = {Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show how these layers can be implemented using three basic operations: pooling, broadcasting, and fully connected layers applied to the input in an appropriate manner. We demonstrate the effectiveness of our architecture and its advantages over natural baselines in a variety of learning tasks.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Navon, Aviv and Shamsian, Aviv and Achituve, Idan and Fetaya, Ethan and Chechik, Gal and Maron, Haggai},
	month = jan,
	year = {2023},
	note = {Issue: arXiv:2301.12780
arXiv:2301.12780 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{garrigos_handbook_2023,
	title = {Handbook of {Convergence} {Theorems} for ({Stochastic}) {Gradient} {Methods}},
	url = {http://arxiv.org/abs/2301.11235},
	doi = {10.48550/arXiv.2301.11235},
	abstract = {This is a handbook of simple proofs of the convergence of gradient and stochastic gradient descent type methods. We consider functions that are Lipschitz, smooth, convex, strongly convex, and/or Polyak-\{{\textbackslash}L\}ojasiewicz functions. Our focus is on ``good proofs'' that are also simple. Each section can be consulted separately. We start with proofs of gradient descent, then on stochastic variants, including minibatching and momentum. Then move on to nonsmooth problems with the subgradient method, the proximal gradient descent and their stochastic variants. Our focus is on global convergence rates and complexity rates. Some slightly less common proofs found here include that of SGD (Stochastic gradient descent) with a proximal step, with momentum, and with mini-batching without replacement.},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Garrigos, Guillaume and Gower, Robert M.},
	month = jan,
	year = {2023},
	note = {Issue: arXiv:2301.11235
arXiv:2301.11235 [math]},
	keywords = {65K05, 68T99, G.1.6, Mathematics - Optimization and Control},
}

@misc{raissi_open_2023,
	title = {Open {Problems} in {Applied} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2301.11316},
	doi = {10.48550/arXiv.2301.11316},
	abstract = {This work formulates the machine learning mechanism as a bi-level optimization problem. The inner level optimization loop entails minimizing a properly chosen loss function evaluated on the training data. This is nothing but the well-studied training process in pursuit of optimal model parameters. The outer level optimization loop is less well-studied and involves maximizing a properly chosen performance metric evaluated on the validation data. This is what we call the "iteration process", pursuing optimal model hyper-parameters. Among many other degrees of freedom, this process entails model engineering (e.g., neural network architecture design) and management, experiment tracking, dataset versioning and augmentation. The iteration process could be automated via Automatic Machine Learning (AutoML) or left to the intuitions of machine learning students, engineers, and researchers. Regardless of the route we take, there is a need to reduce the computational cost of the iteration step and as a direct consequence reduce the carbon footprint of developing artificial intelligence algorithms. Despite the clean and unified mathematical formulation of the iteration step as a bi-level optimization problem, its solutions are case specific and complex. This work will consider such cases while increasing the level of complexity from supervised learning to semi-supervised, self-supervised, unsupervised, few-shot, federated, reinforcement, and physics-informed learning. As a consequence of this exercise, this proposal surfaces a plethora of open problems in the field, many of which can be addressed in parallel.},
	urldate = {2023-01-28},
	publisher = {arXiv},
	author = {Raissi, Maziar},
	month = jan,
	year = {2023},
	note = {Issue: arXiv:2301.11316
arXiv:2301.11316 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{schmidt_descending_2021,
	title = {Descending through a {Crowded} {Valley} - {Benchmarking} {Deep} {Learning} {Optimizers}},
	url = {http://arxiv.org/abs/2007.01547},
	abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of ﬁfteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than 50,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, ﬁxed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a signiﬁcantly reduced subset of speciﬁc optimizers and parameter choices that generally lead to competitive results in our experiments: ADAM remains a strong contender, with newer methods failing to signiﬁcantly and consistently outperform it. Our open-sourced results1 are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
	language = {en},
	urldate = {2023-01-24},
	publisher = {arXiv},
	author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
	month = aug,
	year = {2021},
	note = {Issue: arXiv:2007.01547
arXiv:2007.01547 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lam_cache_1991,
	title = {The cache performance and optimizations of blocked algorithms},
	volume = {26},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/106973.106981},
	doi = {10.1145/106973.106981},
	number = {4},
	urldate = {2023-01-17},
	journal = {ACM SIGPLAN Notices},
	author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
	year = {1991},
	note = {Number: 4},
	pages = {63--74},
}

@misc{liu_monolith_2022,
	title = {Monolith: {Real} {Time} {Recommendation} {System} {With} {Collisionless} {Embedding} {Table}},
	shorttitle = {Monolith},
	url = {http://arxiv.org/abs/2209.07663},
	doi = {10.48550/arXiv.2209.07663},
	abstract = {Building a scalable and real-time recommendation system is vital for many businesses driven by time-sensitive customer feedback, such as short-videos ranking or online ads. Despite the ubiquitous adoption of production-scale deep learning frameworks like TensorFlow or PyTorch, these general-purpose frameworks fall short of business demands in recommendation scenarios for various reasons: on one hand, tweaking systems based on static parameters and dense computations for recommendation with dynamic and sparse features is detrimental to model quality; on the other hand, such frameworks are designed with batch-training stage and serving stage completely separated, preventing the model from interacting with customer feedback in real-time. These issues led us to reexamine traditional approaches and explore radically different design choices. In this paper, we present Monolith, a system tailored for online training. Our design has been driven by observations of our application workloads and production environment that reflects a marked departure from other recommendations systems. Our contributions are manifold: first, we crafted a collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint; second, we provide an production-ready online training architecture with high fault-tolerance; finally, we proved that system reliability could be traded-off for real-time learning. Monolith has successfully landed in the BytePlus Recommend product.},
	urldate = {2022-10-31},
	publisher = {arXiv},
	author = {Liu, Zhuoran and Zou, Leqi and Zou, Xuan and Wang, Caihua and Zhang, Biao and Tang, Da and Zhu, Bolin and Zhu, Yijie and Wu, Peng and Wang, Ke and Cheng, Youlong},
	month = sep,
	year = {2022},
	note = {Issue: arXiv:2209.07663
arXiv:2209.07663 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@techreport{raschka_model_2020,
	title = {Model {Evaluation}, {Model} {Selection}, and {Algorithm} {Selection} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1811.12808},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	number = {arXiv:1811.12808},
	urldate = {2022-05-13},
	institution = {arXiv},
	author = {Raschka, Sebastian},
	month = nov,
	year = {2020},
	doi = {10.48550/arXiv.1811.12808},
	note = {Issue: arXiv:1811.12808
arXiv:1811.12808 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ainsworth_git_2022,
	title = {Git {Re}-{Basin}: {Merging} {Models} modulo {Permutation} {Symmetries}},
	shorttitle = {Git {Re}-{Basin}},
	url = {http://arxiv.org/abs/2209.04836},
	doi = {10.48550/arXiv.2209.04836},
	abstract = {The success of deep learning is thanks to our ability to solve certain massive non-convex optimization problems with relative ease. Despite non-convex optimization being NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin, after accounting for all possible permutation symmetries of hidden units. We introduce three algorithms to permute the units of one model to bring them into alignment with units of a reference model. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10 and CIFAR-100. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity across a variety of models and datasets. Finally, we discuss shortcomings of a single basin theory, including a counterexample to the linear mode connectivity hypothesis.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddhartha},
	month = sep,
	year = {2022},
	note = {Issue: arXiv:2209.04836
arXiv:2209.04836 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
